{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"q2-R_lnK4cvQ"},"outputs":[],"source":["import pandas as pd\n","from konlpy.tag import Mecab\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from collections import Counter\n","from konlpy.tag import Mecab\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize_sentense(text): # 형태소 변환 함수\n","    mecab=Mecab()\n","    return mecab.morphs(text)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172},"executionInfo":{"elapsed":12,"status":"error","timestamp":1665556170638,"user":{"displayName":"r kim","userId":"17339140456772886711"},"user_tz":-540},"id":"7Kcc-Kc84e45","outputId":"a5e381b3-6cec-4d9d-edff-9e14d7cb192f"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-74cfddc8190a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ᅲᅲᅲᅲᅲᅲᅲᅲᅲᅲᅲᅲᅲᅲ' is not defined"]}],"source":["raw_data = pd.read_csv('spam.csv', index_col = 0) # 데이터 기본 전처리\n","raw_data.reset_index(drop = True, inplace = True)\n","raw_data.dropna(axis = 0)"]},{"cell_type":"markdown","metadata":{},"source":["# 기본 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2ChgftT4g6F"},"outputs":[],"source":["raw_data['token'] = raw_data['text'].apply(tokenize_sentense) # text 형태소 변환\n","raw_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxamGkzf4g3r"},"outputs":[],"source":["raw_data['label'] = raw_data['label'].map({'spam' : 1, 'ham' : 0}) # label열 가변수화\n","raw_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfXLiIXv4g1T"},"outputs":[],"source":["# 기본 전처리가 끝난 데이터를 저장\n","# raw_data.to_csv('spam_preprocessing.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqIUrj_j4gy6"},"outputs":[],"source":["raw_data.drop_duplicates(subset=['text'], inplace=True) # text열 중복 제거\n","print('총 샘플의 수 :',len(raw_data))\n","train_data, test_data = train_test_split(raw_data, test_size = 0.3, random_state = 2022) # random state 값은 상관 X\n","print('훈련용 데이터의 개수 :', len(train_data))\n","print('테스트용 데이터의 개수 :', len(test_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NE5FadSU4gwi"},"outputs":[],"source":["train_data['label'].value_counts().plot(kind = 'bar') # 1 (spam), 0 (ham)의 개수 파악"]},{"cell_type":"markdown","metadata":{},"source":["# 정수 임베딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeXhyJr-4guS"},"outputs":[],"source":["X_train = train_data['token'].values # dict에서 값을 뽑음, 아마 토큰화된 리스트 속 리스트의 단어 하나하나를 뽑는 것 같음\n","y_train = train_data['label'].values\n","X_test= test_data['token'].values\n","y_test = test_data['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARgyHx6f4gr7"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train) # 정수 임베딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQKeISFX4gpN"},"outputs":[],"source":["threshold = 2 # 희귀단어 제거 (등장 횟수가 2회 미만인 단어 제거)\n","total_cnt = len(tokenizer.word_index)\n","rare_cnt = 0\n","total_freq = 0\n","rare_freq = 0\n","\n","\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3kPu5NX4gma"},"outputs":[],"source":["# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n","# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2 (OOV = out of vocabulary, 컴퓨터가 이해할 수 없는 단어)\n","vocab_size = total_cnt - rare_cnt + 2\n","print('단어 집합의 크기 :',vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPvF2XEz4s6g"},"outputs":[],"source":["tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') # 단어 집합의 크기보다 큰 숫자가 부여된 단어들은 전부 OOV로 변환\n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTDDsLPw4s4u"},"outputs":[],"source":["print(X_train[:3]) # 정수 임베딩 적용 확인"]},{"cell_type":"markdown","metadata":{},"source":["# 패딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3Kay3DV4s2q"},"outputs":[],"source":["print('메세지의 최대 길이 :',max(len(message) for message in X_train)) # 전체 데이터의 분포 확인\n","print('메세지의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n","plt.hist([len(message) for message in X_train], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zewm5OG4s00"},"outputs":[],"source":["def below_threshold_len(max_len, nested_list): # 패딩을 max_len으로 했을때 얼마나 많은 샘플을 보전할 수 있는지 확인\n","  count = 0\n","  for sentence in nested_list:\n","    if(len(sentence) <= max_len):\n","        count = count + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n","\n","max_len = 70\n","below_threshold_len(max_len, X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train = pad_sequences(X_train, maxlen=max_len) # max_len으로 train, test padding\n","X_test = pad_sequences(X_test, maxlen=max_len)"]},{"cell_type":"markdown","metadata":{},"source":["# Bahdanau attention\n","- 바다나우 어텐션은 RNN의 근본적인 문제, time step을 지나며 데이터가 손실되는 것을 해결하기 위해 RNN의 모든 은닉 상태를 다시 한 번 참고해 주는 방법임\n","- 어텐션 메커니즘에는 기본적으로 루옹 어텐션이 있고 바다나우가 조금 더 복잡한 형태임 (바다나우 어텐션 이론 참고 : https://wikidocs.net/73161)\n","- 루옹 어텐션, 닷 프로덕트(루옹 어텐션 중 하나)와의 차이점 (https://velog.io/@guide333/Attention-%EC%A0%95%EB%A6%AC#%EB%B0%94%EB%8B%A4%EB%82%98%EC%9A%B0-%EC%96%B4%ED%85%90%EC%85%98%EA%B3%BC-%EB%A3%A8%EC%98%B9-%EC%96%B4%ED%85%90%EC%85%98%EC%9D%98-%EC%B0%A8%EC%9D%B4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MeENTuZD4syz"},"outputs":[],"source":["import tensorflow as tf\n","\n","class BahdanauAttention(tf.keras.Model): # Bahdanau attention 함수\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = Dense(units)\n","    self.W2 = Dense(units)\n","    self.V = Dense(1)\n","\n","  def call(self, values, query): # 단, key와 value는 같음\n","    # query shape == (batch_size, hidden size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"]},{"cell_type":"markdown","metadata":{},"source":["# 모델링"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ARXfWfp4swy"},"outputs":[],"source":["from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n","from tensorflow.keras import Input, Model\n","from tensorflow.keras import optimizers\n","import os\n","\n","sequence_input = Input(shape=(max_len,), dtype='int32') # 10000개의 단어를 128차원의 벡터로 임베딩하도록 설계\n","embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)\n","\n","# lstm 두 층을 쌓음\n","\n","lstm = Bidirectional(LSTM(128, dropout=0.5, return_sequences = True))(embedded_sequences)\n","# lstm 첫번째 층, 두 번째 층을 위에 쌓을 예정이므로 return_sequences를 True로 해주어야 함 (아래층이 forward 방향이고 위층이 backward 방향이라는 것을 뜻하는 것 같음)\n","# 양방향 lstm 구조 참고 (https://wegonnamakeit.tistory.com/25) - 그림 8-30\n","lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n","  (LSTM(128, dropout=0.5, return_sequences=True, return_state=True))(lstm)\n","# lstm 두 번째 층\n","print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n","# 각 상태의 shape 출력"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jgEhA8r4gfT"},"outputs":[],"source":["state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n","state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n","\n","attention = BahdanauAttention(64) # 가중치 크기 정의\n","context_vector, attention_weights = attention(lstm, state_h)\n","# 컨텍스트 벡터는 은닉 상태와 어텐션 가중치를 가중합하여 구할 수 있음 (https://velog.io/@eunddodi/바다나우-어텐션) - 3)\n","dense1 = Dense(20, activation=\"relu\")(context_vector) # 컨텍스트 벡터를 dense층에 통과\n","dropout = Dropout(0.5)(dense1)\n","output = Dense(1, activation=\"sigmoid\")(dropout) # 분류 문제이므로 sigmoid, 출력은 1\n","model = Model(inputs=sequence_input, outputs=output)\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","mc = ModelCheckpoint('best_model_bahdanau.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","# 모델 중간저장\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","history = model.fit(X_train, y_train, epochs = 15, callbacks=[es, mc], batch_size = 256, validation_split=0.2, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lvDTPce4zb5"},"outputs":[],"source":["test_data = pd.read_csv('spam_test_text.csv')\n","\n","test_data['token'] = test_data['text'].apply(tokenize_sentense) # test 데이터 토큰화\n","\n","test_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ele6kCe94zXe"},"outputs":[],"source":["test = tokenizer.texts_to_sequences(test_data['token'].values) # test 데이터 정수 임베딩\n","test = pad_sequences(test, maxlen=max_len) # test 데이터 패딩\n","test[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkHG-Yks4zRc"},"outputs":[],"source":["score = model.predict(test) # 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iW7okCaD5A-l"},"outputs":[],"source":["score = pd.DataFrame(score)\n","score.columns = ['result1']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlqILmdy5A8i"},"outputs":[],"source":["score.loc[score['result1'] >= 0.5, 'result2'] = 'spam' # 0.5 이상은 1\n","score.loc[score['result1'] < 0.5, 'result2'] = 'ham' # 0.5 미만은 0\n","df = pd.DataFrame()\n","df['id'] = list(range(9896))\n","df['label'] = score['result2']\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7IeWDkn5A6p"},"outputs":[],"source":["df.to_csv('result_4.csv', index = False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPDA5YtpBo2iMiv3dW23uBZ","collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}

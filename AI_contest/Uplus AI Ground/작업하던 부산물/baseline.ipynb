{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOKnNSACYU0-"
   },
   "source": [
    "# LGU+ 경진대회 - 베이스라인  \n",
    "- [Neural Collaborative Filtering(NCF)](https://arxiv.org/pdf/1708.05031.pdf) 논문의 NeuMF를 참고하여 side-information을 결합한 모델을 PyTorch로 구현\n",
    "- 구현된 모델의 검증 데이터셋과 리더보드의 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차 \n",
    "- 데이터 전처리 \n",
    "    - 기본 설정\n",
    "    - 데이터 불러오기 \n",
    "    - 학습 및 검증 데이터 생성 \n",
    "- NeuMF 구현    \n",
    "    - 모델 구현 \n",
    "    - 학습 및 추론 코드 구현\n",
    "- 모델 학습 \n",
    "    - 하이퍼파라미터 설정 & 최적화 기법 설정\n",
    "    - 모델 학습 \n",
    "    - 학습 과정 시각화 \n",
    "- 제출 \n",
    "    - 모든 유저에 대해 추천 결과 생성\n",
    "    - 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07lHW5CAYU1F"
   },
   "source": [
    "## 데이터 전처리\n",
    "### 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T06:46:48.100268Z",
     "iopub.status.busy": "2022-11-17T06:46:48.099707Z",
     "iopub.status.idle": "2022-11-17T06:46:55.164947Z",
     "shell.execute_reply": "2022-11-17T06:46:55.163737Z",
     "shell.execute_reply.started": "2022-11-17T06:46:48.100237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotnine\n",
      "  Downloading plotnine-0.10.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from plotnine) (1.23.1)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.9/dist-packages (from plotnine) (1.4.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from plotnine) (1.8.1)\n",
      "Collecting mizani>=0.8.1\n",
      "  Downloading mizani-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from plotnine) (3.5.2)\n",
      "Collecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting statsmodels>=0.13.2\n",
      "  Downloading statsmodels-0.13.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (1.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.5.0->plotnine) (4.34.4)\n",
      "Collecting palettable\n",
      "  Downloading palettable-3.3.0-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.5->plotnine) (2022.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.1->plotnine) (1.14.0)\n",
      "Installing collected packages: palettable, patsy, statsmodels, mizani, plotnine\n",
      "Successfully installed mizani-0.8.1 palettable-3.3.0 patsy-0.5.3 plotnine-0.10.1 statsmodels-0.13.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T06:46:55.166881Z",
     "iopub.status.busy": "2022-11-17T06:46:55.166611Z",
     "iopub.status.idle": "2022-11-17T06:46:56.604427Z",
     "shell.execute_reply": "2022-11-17T06:46:56.603596Z",
     "shell.execute_reply.started": "2022-11-17T06:46:55.166855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: lxml 4.5.0\n",
      "Uninstalling lxml-4.5.0:\n",
      "  Successfully uninstalled lxml-4.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip uninstall lxml -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T06:46:57.398108Z",
     "iopub.status.busy": "2022-11-17T06:46:57.397749Z",
     "iopub.status.idle": "2022-11-17T06:47:00.793107Z",
     "shell.execute_reply": "2022-11-17T06:47:00.791996Z",
     "shell.execute_reply.started": "2022-11-17T06:46:57.398081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iHstgRfiYU1G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 패키지 로드\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os, random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import plotnine\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 \n",
    "class cfg: \n",
    "    gpu_idx = 0\n",
    "    device = torch.device(\"cuda:{}\".format(gpu_idx) if torch.cuda.is_available() else \"cpu\") # cuda가 사용가능한지 보여주는거\n",
    "    top_k = 25\n",
    "    seed = 42\n",
    "    neg_ratio = 100\n",
    "    test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5GrHkU7AYU1I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 시드 고정 \n",
    "def seed_everything(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "seed_everything(cfg.seed) # cfg에 있는 seed를 뽑아서 random_seed에 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "data_path = 'E:\\\\aivle\\\\2022_uplus_ai_ground\\\\data'\n",
    "saved_path = 'E:\\\\aivle\\\\2022_uplus_ai_ground\\\\data\\\\saved'\n",
    "output_path = 'E:\\\\aivle\\\\2022_uplus_ai_ground\\\\data\\\\submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huw6BwpTYU1J"
   },
   "source": [
    "### 데이터 불러오기\n",
    "- history_data : 시청 시작 데이터\n",
    "- profile_data : 프로필 정보 \n",
    "- meta_data : 콘텐츠 일반 메타 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A23qw5LXYU1M"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기 \n",
    "history_df = pd.read_csv(os.path.join(data_path, 'history_data.csv'), encoding='utf-8')\n",
    "profile_df = pd.read_csv(os.path.join(data_path, 'profile_data.csv'), encoding='utf-8')\n",
    "meta_df = pd.read_csv(os.path.join(data_path, 'meta_data.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRnh1wEiYU1M"
   },
   "source": [
    "### 학습 및 검증 데이터 생성 \n",
    "- train : 시청 이력의 80%를 사용 \n",
    "- valid : 시청 이력의 20%를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_id</th>\n",
       "      <th>ss_id</th>\n",
       "      <th>log_time</th>\n",
       "      <th>album_id</th>\n",
       "      <th>payment</th>\n",
       "      <th>continuous_play</th>\n",
       "      <th>short_trailer</th>\n",
       "      <th>watch_time</th>\n",
       "      <th>total_time</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115653</td>\n",
       "      <td>20220301115719</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115653</td>\n",
       "      <td>20220301115809</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115653</td>\n",
       "      <td>20220301115958</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115653</td>\n",
       "      <td>20220301120118</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115653</td>\n",
       "      <td>20220301120229</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892789</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155091</td>\n",
       "      <td>20220427155653</td>\n",
       "      <td>381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892790</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155091</td>\n",
       "      <td>20220427155694</td>\n",
       "      <td>381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892791</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155839</td>\n",
       "      <td>20220427155826</td>\n",
       "      <td>125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892792</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155706</td>\n",
       "      <td>20220427155836</td>\n",
       "      <td>125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892793</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155839</td>\n",
       "      <td>20220427155897</td>\n",
       "      <td>381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1792067 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        profile_id           ss_id        log_time  album_id  payment  \\\n",
       "0                3  20220301115653  20220301115719        15      0.0   \n",
       "1                3  20220301115653  20220301115809        16      0.0   \n",
       "2                3  20220301115653  20220301115958        17      0.0   \n",
       "3                3  20220301115653  20220301120118        18      0.0   \n",
       "4                3  20220301115653  20220301120229        19      0.0   \n",
       "...            ...             ...             ...       ...      ...   \n",
       "892789       33032  20220427155091  20220427155653       381      0.0   \n",
       "892790       33032  20220427155091  20220427155694       381      0.0   \n",
       "892791       33032  20220427155839  20220427155826       125      0.0   \n",
       "892792       33032  20220427155706  20220427155836       125      0.0   \n",
       "892793       33032  20220427155839  20220427155897       381      0.0   \n",
       "\n",
       "        continuous_play  short_trailer  watch_time  total_time  rating  \n",
       "0                     1            0.0         0.0         0.0       1  \n",
       "1                     1            0.0         0.0         0.0       1  \n",
       "2                     1            0.0         0.0         0.0       1  \n",
       "3                     1            0.0         0.0         0.0       1  \n",
       "4                     1            0.0         0.0         0.0       1  \n",
       "...                 ...            ...         ...         ...     ...  \n",
       "892789                1            0.0       463.0       464.0       1  \n",
       "892790                1            0.0       462.0       463.0       1  \n",
       "892791                0            0.0        10.0       520.0       1  \n",
       "892792                0            0.0         6.0       521.0       1  \n",
       "892793                1            0.0       462.0       464.0       1  \n",
       "\n",
       "[1792067 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33033\n",
      "25917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'device',\n",
       " 'gpu_idx',\n",
       " 'n_items',\n",
       " 'n_users',\n",
       " 'neg_ratio',\n",
       " 'seed',\n",
       " 'test_size',\n",
       " 'top_k']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 (중복제거) \n",
    "# 참고 : drop_duplicates의 subset을 무엇으로 구성하냐에 따라서 제거되는 항목들이 다름 \n",
    "# ex) 'profile_id', 'album_id' : 중복된 시청이력 모두 제거 / 'profile_id', 'album_id', 'log_time' : 같은 시간에 시청한 이력만 제거 \n",
    "# data = history_df[['profile_id', 'log_time', 'album_id']].drop_duplicates(subset=['profile_id', 'album_id', 'log_time']).sort_values(by = ['profile_id', 'log_time']).reset_index(drop = True)\n",
    "data = pd.read_csv(os.path.join(data_path, 'test_data1.csv'), encoding='utf-8', index_col = 0)\n",
    "data['rating'] = 1\n",
    "display(data)\n",
    "\n",
    "cfg.n_users = data.profile_id.max()+1 # 유저의 수는 profile_id + 1\n",
    "cfg.n_items = data.album_id.max()+1 # 마찬가지로 item의 수는 album_id + 1\n",
    "\n",
    "print(cfg.n_users)\n",
    "print(cfg.n_items)\n",
    "\n",
    "dir(cfg) # cfg 객체의 정보 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: (1433653, 10)\n",
      "검증 데이터 크기: (358414, 10)\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 검증 데이터 분리\n",
    "train, valid = train_test_split(\n",
    "    data, test_size=cfg.test_size, random_state=cfg.seed,\n",
    ") # cfg에 있던 test_size , seed를 가져와서 넣어줌\n",
    "print('학습 데이터 크기:', train.shape)\n",
    "print('검증 데이터 크기:', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4c8ad9434c4b33a45d140e7fa15da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1433653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 형태: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25907</th>\n",
       "      <th>25908</th>\n",
       "      <th>25909</th>\n",
       "      <th>25910</th>\n",
       "      <th>25911</th>\n",
       "      <th>25912</th>\n",
       "      <th>25913</th>\n",
       "      <th>25914</th>\n",
       "      <th>25915</th>\n",
       "      <th>25916</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33028</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33029</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33030</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33031</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33032</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33033 rows × 25917 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9      \\\n",
       "0        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "33028    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33029    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33030    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33031    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33032    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...  25907  25908  25909  25910  25911  25912  25913  25914  25915  \\\n",
       "0      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "33028  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33029  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33030  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33031  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "33032  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       25916  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "33028    0.0  \n",
       "33029    0.0  \n",
       "33030    0.0  \n",
       "33031    0.0  \n",
       "33032    0.0  \n",
       "\n",
       "[33033 rows x 25917 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matrix 형태로 변환 \n",
    "train = train.to_numpy() # train data를 넘파이 형태로 변환\n",
    "matrix = sparse.lil_matrix((cfg.n_users, cfg.n_items)) # lil_matrix((M, N), [dtype]) : dtype은 선택, 모양이 m, n인 빈 행렬 구성\n",
    "for (p, _, _, i, pay, c, s, w, t, r) in tqdm(train): # p = profile_id 첫번째 행, i = album_id 첫번째 행, r = rating 첫번째 행\n",
    "    matrix[p, i] = r\n",
    "    # matrix[pay, c] = r\n",
    "    \n",
    "# print(matrix)\n",
    "# display(train)\n",
    "train = sparse.csr_matrix(matrix) # 희소 행렬로 변환 --> 0을 제외한 데이터를 저장한다, \n",
    "train = train.toarray() # array로 다시 변환\n",
    "print(\"train 형태: \\n\", train)\n",
    "print(train.dtype)\n",
    "data_df = pd.DataFrame(train)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "33033\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(type(train))\n",
    "print(len(train))\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id 3의 age 정보 : 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'age': {3: 5,\n",
       "  5: 5,\n",
       "  7: 9,\n",
       "  12: 6,\n",
       "  16: 12,\n",
       "  19: 3,\n",
       "  20: 7,\n",
       "  22: 4,\n",
       "  24: 5,\n",
       "  26: 7,\n",
       "  27: 7,\n",
       "  30: 2,\n",
       "  31: 6,\n",
       "  33: 7,\n",
       "  35: 7,\n",
       "  55: 8,\n",
       "  56: 11,\n",
       "  59: 7,\n",
       "  62: 5,\n",
       "  63: 9,\n",
       "  66: 7,\n",
       "  68: 5,\n",
       "  74: 2,\n",
       "  77: 9,\n",
       "  80: 7,\n",
       "  89: 7,\n",
       "  94: 7,\n",
       "  96: 3,\n",
       "  101: 4,\n",
       "  102: 3,\n",
       "  103: 7,\n",
       "  109: 4,\n",
       "  110: 8,\n",
       "  115: 4,\n",
       "  116: 7,\n",
       "  118: 6,\n",
       "  121: 6,\n",
       "  122: 3,\n",
       "  124: 8,\n",
       "  127: 7,\n",
       "  129: 6,\n",
       "  130: 10,\n",
       "  136: 2,\n",
       "  137: 7,\n",
       "  139: 9,\n",
       "  144: 2,\n",
       "  150: 6,\n",
       "  152: 6,\n",
       "  156: 4,\n",
       "  165: 7,\n",
       "  170: 8,\n",
       "  172: 7,\n",
       "  176: 3,\n",
       "  179: 5,\n",
       "  181: 5,\n",
       "  182: 8,\n",
       "  187: 2,\n",
       "  192: 5,\n",
       "  193: 9,\n",
       "  195: 6,\n",
       "  198: 8,\n",
       "  201: 4,\n",
       "  207: 4,\n",
       "  209: 6,\n",
       "  211: 8,\n",
       "  214: 6,\n",
       "  220: 8,\n",
       "  228: 9,\n",
       "  229: 3,\n",
       "  230: 2,\n",
       "  233: 6,\n",
       "  240: 2,\n",
       "  248: 5,\n",
       "  251: 6,\n",
       "  255: 9,\n",
       "  258: 10,\n",
       "  260: 6,\n",
       "  261: 4,\n",
       "  264: 4,\n",
       "  279: 4,\n",
       "  290: 8,\n",
       "  292: 2,\n",
       "  293: 6,\n",
       "  300: 6,\n",
       "  302: 6,\n",
       "  303: 2,\n",
       "  304: 5,\n",
       "  307: 3,\n",
       "  309: 4,\n",
       "  316: 6,\n",
       "  317: 5,\n",
       "  321: 6,\n",
       "  326: 8,\n",
       "  327: 4,\n",
       "  335: 6,\n",
       "  338: 12,\n",
       "  345: 7,\n",
       "  346: 1,\n",
       "  349: 5,\n",
       "  350: 1,\n",
       "  358: 8,\n",
       "  361: 5,\n",
       "  363: 3,\n",
       "  364: 9,\n",
       "  369: 2,\n",
       "  370: 7,\n",
       "  372: 4,\n",
       "  373: 4,\n",
       "  384: 2,\n",
       "  390: 6,\n",
       "  395: 8,\n",
       "  397: 6,\n",
       "  399: 4,\n",
       "  402: 6,\n",
       "  407: 8,\n",
       "  408: 8,\n",
       "  411: 7,\n",
       "  414: 4,\n",
       "  415: 9,\n",
       "  417: 8,\n",
       "  420: 3,\n",
       "  423: 3,\n",
       "  436: 4,\n",
       "  444: 8,\n",
       "  447: 9,\n",
       "  448: 9,\n",
       "  450: 5,\n",
       "  452: 11,\n",
       "  457: 6,\n",
       "  459: 13,\n",
       "  462: 4,\n",
       "  463: 2,\n",
       "  464: 3,\n",
       "  465: 9,\n",
       "  471: 7,\n",
       "  472: 7,\n",
       "  478: 3,\n",
       "  479: 8,\n",
       "  486: 4,\n",
       "  489: 8,\n",
       "  491: 6,\n",
       "  493: 4,\n",
       "  495: 3,\n",
       "  503: 4,\n",
       "  514: 6,\n",
       "  518: 6,\n",
       "  525: 2,\n",
       "  527: 3,\n",
       "  530: 9,\n",
       "  533: 8,\n",
       "  534: 5,\n",
       "  537: 6,\n",
       "  539: 6,\n",
       "  541: 6,\n",
       "  546: 3,\n",
       "  551: 10,\n",
       "  561: 9,\n",
       "  567: 2,\n",
       "  570: 5,\n",
       "  572: 8,\n",
       "  574: 5,\n",
       "  579: 3,\n",
       "  580: 3,\n",
       "  581: 5,\n",
       "  582: 3,\n",
       "  587: 6,\n",
       "  590: 8,\n",
       "  594: 7,\n",
       "  595: 4,\n",
       "  603: 4,\n",
       "  604: 12,\n",
       "  607: 3,\n",
       "  610: 5,\n",
       "  613: 6,\n",
       "  616: 3,\n",
       "  621: 5,\n",
       "  622: 4,\n",
       "  626: 2,\n",
       "  629: 11,\n",
       "  637: 3,\n",
       "  645: 3,\n",
       "  647: 5,\n",
       "  648: 5,\n",
       "  649: 4,\n",
       "  653: 9,\n",
       "  654: 4,\n",
       "  664: 4,\n",
       "  670: 5,\n",
       "  671: 8,\n",
       "  673: 5,\n",
       "  695: 6,\n",
       "  700: 3,\n",
       "  702: 8,\n",
       "  704: 4,\n",
       "  705: 5,\n",
       "  712: 3,\n",
       "  717: 7,\n",
       "  721: 7,\n",
       "  725: 5,\n",
       "  726: 4,\n",
       "  727: 6,\n",
       "  729: 2,\n",
       "  734: 4,\n",
       "  735: 2,\n",
       "  736: 7,\n",
       "  738: 7,\n",
       "  740: 4,\n",
       "  743: 8,\n",
       "  748: 8,\n",
       "  749: 3,\n",
       "  753: 7,\n",
       "  754: 3,\n",
       "  758: 9,\n",
       "  759: 1,\n",
       "  762: 6,\n",
       "  764: 5,\n",
       "  769: 5,\n",
       "  770: 6,\n",
       "  777: 7,\n",
       "  779: 5,\n",
       "  782: 7,\n",
       "  788: 3,\n",
       "  793: 6,\n",
       "  798: 6,\n",
       "  804: 7,\n",
       "  805: 5,\n",
       "  808: 6,\n",
       "  810: 4,\n",
       "  814: 3,\n",
       "  815: 3,\n",
       "  816: 4,\n",
       "  824: 4,\n",
       "  826: 9,\n",
       "  832: 6,\n",
       "  848: 5,\n",
       "  854: 8,\n",
       "  855: 6,\n",
       "  857: 5,\n",
       "  861: 3,\n",
       "  862: 5,\n",
       "  863: 4,\n",
       "  874: 8,\n",
       "  879: 2,\n",
       "  884: 6,\n",
       "  891: 5,\n",
       "  893: 3,\n",
       "  896: 3,\n",
       "  897: 6,\n",
       "  908: 11,\n",
       "  910: 6,\n",
       "  916: 3,\n",
       "  925: 3,\n",
       "  929: 4,\n",
       "  931: 7,\n",
       "  935: 1,\n",
       "  936: 7,\n",
       "  943: 3,\n",
       "  947: 7,\n",
       "  948: 7,\n",
       "  950: 5,\n",
       "  951: 8,\n",
       "  953: 6,\n",
       "  960: 6,\n",
       "  964: 6,\n",
       "  970: 4,\n",
       "  973: 6,\n",
       "  975: 3,\n",
       "  977: 4,\n",
       "  980: 9,\n",
       "  981: 8,\n",
       "  985: 2,\n",
       "  988: 4,\n",
       "  989: 6,\n",
       "  990: 3,\n",
       "  993: 1,\n",
       "  994: 9,\n",
       "  999: 4,\n",
       "  1002: 5,\n",
       "  1007: 9,\n",
       "  1012: 6,\n",
       "  1020: 5,\n",
       "  1024: 1,\n",
       "  1026: 7,\n",
       "  1027: 3,\n",
       "  1028: 4,\n",
       "  1033: 8,\n",
       "  1038: 6,\n",
       "  1039: 4,\n",
       "  1040: 7,\n",
       "  1042: 7,\n",
       "  1043: 9,\n",
       "  1046: 4,\n",
       "  1048: 4,\n",
       "  1049: 2,\n",
       "  1056: 4,\n",
       "  1058: 8,\n",
       "  1063: 6,\n",
       "  1066: 4,\n",
       "  1067: 7,\n",
       "  1069: 4,\n",
       "  1070: 5,\n",
       "  1072: 9,\n",
       "  1077: 5,\n",
       "  1078: 7,\n",
       "  1079: 3,\n",
       "  1080: 4,\n",
       "  1086: 4,\n",
       "  1087: 7,\n",
       "  1089: 6,\n",
       "  1097: 4,\n",
       "  1105: 3,\n",
       "  1106: 7,\n",
       "  1107: 6,\n",
       "  1108: 4,\n",
       "  1110: 3,\n",
       "  1111: 7,\n",
       "  1113: 6,\n",
       "  1116: 8,\n",
       "  1117: 11,\n",
       "  1120: 7,\n",
       "  1123: 2,\n",
       "  1124: 7,\n",
       "  1125: 2,\n",
       "  1130: 9,\n",
       "  1132: 2,\n",
       "  1137: 4,\n",
       "  1138: 4,\n",
       "  1142: 6,\n",
       "  1147: 4,\n",
       "  1148: 8,\n",
       "  1158: 6,\n",
       "  1163: 6,\n",
       "  1164: 7,\n",
       "  1166: 3,\n",
       "  1172: 6,\n",
       "  1175: 3,\n",
       "  1177: 2,\n",
       "  1178: 9,\n",
       "  1192: 5,\n",
       "  1194: 3,\n",
       "  1198: 8,\n",
       "  1208: 6,\n",
       "  1215: 7,\n",
       "  1216: 3,\n",
       "  1220: 5,\n",
       "  1221: 1,\n",
       "  1222: 3,\n",
       "  1223: 10,\n",
       "  1229: 6,\n",
       "  1231: 5,\n",
       "  1233: 9,\n",
       "  1236: 3,\n",
       "  1245: 7,\n",
       "  1259: 7,\n",
       "  1265: 8,\n",
       "  1269: 2,\n",
       "  1270: 4,\n",
       "  1277: 1,\n",
       "  1279: 8,\n",
       "  1282: 3,\n",
       "  1286: 4,\n",
       "  1287: 6,\n",
       "  1298: 4,\n",
       "  1299: 7,\n",
       "  1301: 9,\n",
       "  1306: 2,\n",
       "  1307: 4,\n",
       "  1309: 4,\n",
       "  1313: 4,\n",
       "  1315: 13,\n",
       "  1318: 4,\n",
       "  1322: 8,\n",
       "  1323: 2,\n",
       "  1325: 3,\n",
       "  1332: 3,\n",
       "  1334: 5,\n",
       "  1336: 9,\n",
       "  1337: 6,\n",
       "  1345: 5,\n",
       "  1346: 3,\n",
       "  1347: 8,\n",
       "  1348: 6,\n",
       "  1349: 5,\n",
       "  1352: 8,\n",
       "  1363: 8,\n",
       "  1364: 7,\n",
       "  1378: 9,\n",
       "  1381: 7,\n",
       "  1383: 8,\n",
       "  1385: 7,\n",
       "  1387: 8,\n",
       "  1388: 3,\n",
       "  1391: 8,\n",
       "  1392: 5,\n",
       "  1393: 3,\n",
       "  1395: 6,\n",
       "  1403: 2,\n",
       "  1404: 5,\n",
       "  1407: 7,\n",
       "  1408: 1,\n",
       "  1409: 2,\n",
       "  1410: 3,\n",
       "  1416: 3,\n",
       "  1417: 4,\n",
       "  1423: 4,\n",
       "  1424: 3,\n",
       "  1425: 3,\n",
       "  1428: 4,\n",
       "  1430: 3,\n",
       "  1440: 7,\n",
       "  1441: 12,\n",
       "  1442: 8,\n",
       "  1446: 2,\n",
       "  1450: 6,\n",
       "  1451: 3,\n",
       "  1452: 9,\n",
       "  1453: 1,\n",
       "  1454: 7,\n",
       "  1456: 5,\n",
       "  1457: 5,\n",
       "  1460: 4,\n",
       "  1463: 6,\n",
       "  1466: 7,\n",
       "  1480: 8,\n",
       "  1483: 4,\n",
       "  1484: 3,\n",
       "  1488: 5,\n",
       "  1489: 7,\n",
       "  1490: 3,\n",
       "  1493: 4,\n",
       "  1494: 4,\n",
       "  1495: 9,\n",
       "  1504: 6,\n",
       "  1506: 8,\n",
       "  1515: 6,\n",
       "  1520: 7,\n",
       "  1521: 3,\n",
       "  1522: 4,\n",
       "  1531: 6,\n",
       "  1532: 9,\n",
       "  1534: 9,\n",
       "  1538: 8,\n",
       "  1550: 3,\n",
       "  1551: 4,\n",
       "  1552: 10,\n",
       "  1554: 5,\n",
       "  1555: 8,\n",
       "  1562: 4,\n",
       "  1565: 3,\n",
       "  1571: 8,\n",
       "  1572: 4,\n",
       "  1575: 4,\n",
       "  1576: 7,\n",
       "  1582: 6,\n",
       "  1585: 2,\n",
       "  1591: 5,\n",
       "  1592: 7,\n",
       "  1603: 7,\n",
       "  1605: 3,\n",
       "  1606: 4,\n",
       "  1609: 4,\n",
       "  1610: 5,\n",
       "  1615: 5,\n",
       "  1619: 10,\n",
       "  1621: 6,\n",
       "  1623: 2,\n",
       "  1625: 5,\n",
       "  1631: 3,\n",
       "  1632: 6,\n",
       "  1633: 4,\n",
       "  1635: 5,\n",
       "  1639: 6,\n",
       "  1641: 6,\n",
       "  1645: 6,\n",
       "  1649: 3,\n",
       "  1650: 3,\n",
       "  1652: 6,\n",
       "  1653: 8,\n",
       "  1654: 8,\n",
       "  1667: 5,\n",
       "  1672: 6,\n",
       "  1673: 10,\n",
       "  1679: 4,\n",
       "  1681: 4,\n",
       "  1683: 3,\n",
       "  1684: 5,\n",
       "  1686: 7,\n",
       "  1691: 5,\n",
       "  1697: 4,\n",
       "  1700: 6,\n",
       "  1704: 4,\n",
       "  1705: 3,\n",
       "  1709: 5,\n",
       "  1713: 3,\n",
       "  1719: 5,\n",
       "  1726: 8,\n",
       "  1729: 7,\n",
       "  1738: 7,\n",
       "  1741: 4,\n",
       "  1743: 3,\n",
       "  1745: 3,\n",
       "  1747: 3,\n",
       "  1753: 5,\n",
       "  1763: 4,\n",
       "  1773: 3,\n",
       "  1776: 6,\n",
       "  1778: 6,\n",
       "  1781: 6,\n",
       "  1783: 5,\n",
       "  1786: 4,\n",
       "  1788: 8,\n",
       "  1790: 1,\n",
       "  1791: 11,\n",
       "  1795: 9,\n",
       "  1796: 1,\n",
       "  1799: 4,\n",
       "  1801: 5,\n",
       "  1802: 6,\n",
       "  1803: 6,\n",
       "  1804: 5,\n",
       "  1805: 4,\n",
       "  1806: 6,\n",
       "  1810: 7,\n",
       "  1812: 3,\n",
       "  1813: 1,\n",
       "  1814: 6,\n",
       "  1817: 6,\n",
       "  1821: 5,\n",
       "  1822: 7,\n",
       "  1823: 3,\n",
       "  1826: 3,\n",
       "  1833: 8,\n",
       "  1835: 4,\n",
       "  1846: 4,\n",
       "  1847: 9,\n",
       "  1850: 3,\n",
       "  1852: 7,\n",
       "  1853: 3,\n",
       "  1858: 5,\n",
       "  1871: 1,\n",
       "  1874: 4,\n",
       "  1876: 2,\n",
       "  1878: 3,\n",
       "  1882: 3,\n",
       "  1884: 3,\n",
       "  1888: 5,\n",
       "  1890: 7,\n",
       "  1900: 6,\n",
       "  1902: 5,\n",
       "  1905: 3,\n",
       "  1907: 3,\n",
       "  1908: 3,\n",
       "  1909: 7,\n",
       "  1911: 4,\n",
       "  1916: 4,\n",
       "  1917: 4,\n",
       "  1921: 5,\n",
       "  1922: 7,\n",
       "  1924: 1,\n",
       "  1925: 1,\n",
       "  1932: 3,\n",
       "  1937: 5,\n",
       "  1945: 8,\n",
       "  1948: 2,\n",
       "  1953: 6,\n",
       "  1954: 1,\n",
       "  1957: 7,\n",
       "  1959: 2,\n",
       "  1974: 4,\n",
       "  1981: 2,\n",
       "  1982: 3,\n",
       "  1984: 7,\n",
       "  1987: 5,\n",
       "  1988: 3,\n",
       "  1989: 7,\n",
       "  1992: 9,\n",
       "  1993: 6,\n",
       "  2000: 7,\n",
       "  2006: 4,\n",
       "  2008: 8,\n",
       "  2013: 4,\n",
       "  2014: 4,\n",
       "  2015: 2,\n",
       "  2021: 5,\n",
       "  2022: 3,\n",
       "  2026: 4,\n",
       "  2031: 2,\n",
       "  2032: 5,\n",
       "  2033: 6,\n",
       "  2038: 7,\n",
       "  2040: 6,\n",
       "  2041: 8,\n",
       "  2042: 7,\n",
       "  2046: 8,\n",
       "  2047: 8,\n",
       "  2053: 6,\n",
       "  2054: 10,\n",
       "  2055: 6,\n",
       "  2056: 9,\n",
       "  2057: 6,\n",
       "  2060: 7,\n",
       "  2072: 9,\n",
       "  2075: 4,\n",
       "  2079: 4,\n",
       "  2080: 5,\n",
       "  2081: 3,\n",
       "  2085: 4,\n",
       "  2086: 5,\n",
       "  2089: 6,\n",
       "  2090: 3,\n",
       "  2105: 8,\n",
       "  2106: 6,\n",
       "  2109: 6,\n",
       "  2110: 5,\n",
       "  2118: 4,\n",
       "  2119: 6,\n",
       "  2123: 4,\n",
       "  2124: 6,\n",
       "  2130: 11,\n",
       "  2131: 8,\n",
       "  2143: 9,\n",
       "  2145: 7,\n",
       "  2146: 5,\n",
       "  2148: 3,\n",
       "  2157: 6,\n",
       "  2161: 3,\n",
       "  2162: 4,\n",
       "  2168: 5,\n",
       "  2169: 5,\n",
       "  2171: 1,\n",
       "  2179: 3,\n",
       "  2190: 6,\n",
       "  2196: 5,\n",
       "  2199: 3,\n",
       "  2205: 4,\n",
       "  2206: 7,\n",
       "  2209: 2,\n",
       "  2210: 3,\n",
       "  2211: 5,\n",
       "  2213: 4,\n",
       "  2215: 4,\n",
       "  2217: 6,\n",
       "  2219: 8,\n",
       "  2224: 10,\n",
       "  2225: 5,\n",
       "  2227: 11,\n",
       "  2228: 3,\n",
       "  2229: 4,\n",
       "  2233: 7,\n",
       "  2238: 3,\n",
       "  2239: 3,\n",
       "  2240: 4,\n",
       "  2242: 7,\n",
       "  2243: 1,\n",
       "  2252: 3,\n",
       "  2254: 7,\n",
       "  2259: 2,\n",
       "  2266: 2,\n",
       "  2267: 2,\n",
       "  2270: 5,\n",
       "  2276: 3,\n",
       "  2277: 3,\n",
       "  2278: 4,\n",
       "  2281: 3,\n",
       "  2282: 8,\n",
       "  2293: 3,\n",
       "  2294: 6,\n",
       "  2300: 7,\n",
       "  2303: 12,\n",
       "  2308: 3,\n",
       "  2309: 2,\n",
       "  2310: 3,\n",
       "  2311: 3,\n",
       "  2315: 6,\n",
       "  2318: 3,\n",
       "  2322: 11,\n",
       "  2324: 5,\n",
       "  2326: 6,\n",
       "  2328: 4,\n",
       "  2330: 3,\n",
       "  2332: 9,\n",
       "  2334: 4,\n",
       "  2335: 4,\n",
       "  2336: 6,\n",
       "  2338: 6,\n",
       "  2341: 6,\n",
       "  2342: 5,\n",
       "  2348: 5,\n",
       "  2355: 4,\n",
       "  2362: 4,\n",
       "  2365: 3,\n",
       "  2366: 3,\n",
       "  2367: 3,\n",
       "  2374: 6,\n",
       "  2375: 6,\n",
       "  2379: 3,\n",
       "  2386: 7,\n",
       "  2390: 3,\n",
       "  2391: 4,\n",
       "  2394: 6,\n",
       "  2399: 9,\n",
       "  2401: 7,\n",
       "  2404: 6,\n",
       "  2405: 4,\n",
       "  2409: 5,\n",
       "  2410: 11,\n",
       "  2411: 2,\n",
       "  2414: 4,\n",
       "  2416: 5,\n",
       "  2417: 6,\n",
       "  2423: 5,\n",
       "  2425: 7,\n",
       "  2428: 8,\n",
       "  2431: 8,\n",
       "  2434: 5,\n",
       "  2440: 7,\n",
       "  2441: 6,\n",
       "  2447: 2,\n",
       "  2452: 3,\n",
       "  2460: 6,\n",
       "  2461: 3,\n",
       "  2466: 7,\n",
       "  2480: 3,\n",
       "  2481: 7,\n",
       "  2482: 7,\n",
       "  2485: 6,\n",
       "  2486: 4,\n",
       "  2495: 3,\n",
       "  2499: 4,\n",
       "  2507: 5,\n",
       "  2511: 3,\n",
       "  2514: 7,\n",
       "  2516: 5,\n",
       "  2523: 3,\n",
       "  2528: 8,\n",
       "  2529: 4,\n",
       "  2532: 3,\n",
       "  2533: 3,\n",
       "  2536: 13,\n",
       "  2540: 3,\n",
       "  2544: 9,\n",
       "  2550: 6,\n",
       "  2556: 8,\n",
       "  2559: 3,\n",
       "  2561: 4,\n",
       "  2565: 4,\n",
       "  2566: 7,\n",
       "  2570: 2,\n",
       "  2572: 7,\n",
       "  2573: 4,\n",
       "  2574: 4,\n",
       "  2575: 5,\n",
       "  2577: 3,\n",
       "  2578: 5,\n",
       "  2585: 5,\n",
       "  2595: 4,\n",
       "  2599: 7,\n",
       "  2603: 5,\n",
       "  2612: 5,\n",
       "  2616: 6,\n",
       "  2617: 2,\n",
       "  2622: 7,\n",
       "  2628: 4,\n",
       "  2629: 2,\n",
       "  2638: 7,\n",
       "  2641: 7,\n",
       "  2643: 7,\n",
       "  2648: 7,\n",
       "  2649: 9,\n",
       "  2653: 3,\n",
       "  2655: 1,\n",
       "  2657: 4,\n",
       "  2660: 7,\n",
       "  2664: 8,\n",
       "  2666: 5,\n",
       "  2669: 3,\n",
       "  2676: 8,\n",
       "  2681: 6,\n",
       "  2682: 5,\n",
       "  2683: 4,\n",
       "  2685: 6,\n",
       "  2692: 6,\n",
       "  2697: 7,\n",
       "  2703: 8,\n",
       "  2704: 5,\n",
       "  2705: 3,\n",
       "  2710: 5,\n",
       "  2713: 5,\n",
       "  2717: 7,\n",
       "  2721: 6,\n",
       "  2727: 4,\n",
       "  2730: 6,\n",
       "  2732: 5,\n",
       "  2734: 3,\n",
       "  2737: 3,\n",
       "  2742: 2,\n",
       "  2747: 9,\n",
       "  2748: 3,\n",
       "  2749: 4,\n",
       "  2752: 4,\n",
       "  2756: 6,\n",
       "  2765: 8,\n",
       "  2766: 4,\n",
       "  2768: 5,\n",
       "  2771: 7,\n",
       "  2772: 3,\n",
       "  2776: 2,\n",
       "  2777: 8,\n",
       "  2778: 5,\n",
       "  2788: 1,\n",
       "  2791: 5,\n",
       "  2792: 5,\n",
       "  2794: 2,\n",
       "  2795: 4,\n",
       "  2797: 3,\n",
       "  2798: 7,\n",
       "  2803: 2,\n",
       "  2808: 5,\n",
       "  2809: 5,\n",
       "  2818: 7,\n",
       "  2823: 6,\n",
       "  2828: 2,\n",
       "  2829: 5,\n",
       "  2832: 5,\n",
       "  2835: 6,\n",
       "  2838: 2,\n",
       "  2840: 4,\n",
       "  2842: 3,\n",
       "  2849: 2,\n",
       "  2854: 3,\n",
       "  2862: 4,\n",
       "  2865: 8,\n",
       "  2866: 8,\n",
       "  2869: 3,\n",
       "  2871: 8,\n",
       "  2877: 3,\n",
       "  2878: 8,\n",
       "  2888: 5,\n",
       "  2891: 11,\n",
       "  2893: 5,\n",
       "  2910: 5,\n",
       "  2914: 4,\n",
       "  2919: 3,\n",
       "  2928: 8,\n",
       "  2930: 3,\n",
       "  2934: 1,\n",
       "  2935: 6,\n",
       "  2937: 7,\n",
       "  2938: 4,\n",
       "  2941: 5,\n",
       "  2942: 6,\n",
       "  2943: 9,\n",
       "  2944: 5,\n",
       "  2947: 4,\n",
       "  2951: 4,\n",
       "  2958: 2,\n",
       "  2969: 4,\n",
       "  2972: 7,\n",
       "  2980: 5,\n",
       "  2984: 7,\n",
       "  2985: 1,\n",
       "  2992: 9,\n",
       "  2994: 4,\n",
       "  2997: 4,\n",
       "  3007: 5,\n",
       "  3011: 7,\n",
       "  3013: 2,\n",
       "  3016: 3,\n",
       "  3022: 4,\n",
       "  3023: 8,\n",
       "  3033: 6,\n",
       "  3035: 9,\n",
       "  3045: 2,\n",
       "  3046: 7,\n",
       "  3047: 9,\n",
       "  3053: 8,\n",
       "  3054: 6,\n",
       "  3056: 4,\n",
       "  3058: 3,\n",
       "  3061: 2,\n",
       "  3065: 5,\n",
       "  3068: 4,\n",
       "  3073: 5,\n",
       "  3080: 7,\n",
       "  3081: 7,\n",
       "  3083: 6,\n",
       "  3089: 4,\n",
       "  3090: 6,\n",
       "  3093: 6,\n",
       "  3097: 3,\n",
       "  3100: 5,\n",
       "  3107: 1,\n",
       "  3109: 5,\n",
       "  3116: 7,\n",
       "  3117: 4,\n",
       "  3118: 3,\n",
       "  3121: 11,\n",
       "  3124: 7,\n",
       "  3129: 4,\n",
       "  3137: 6,\n",
       "  3143: 5,\n",
       "  3146: 2,\n",
       "  3153: 4,\n",
       "  3159: 6,\n",
       "  3160: 4,\n",
       "  3162: 8,\n",
       "  3164: 8,\n",
       "  3165: 1,\n",
       "  3168: 2,\n",
       "  3175: 6,\n",
       "  3182: 6,\n",
       "  3188: 3,\n",
       "  3190: 5,\n",
       "  3191: 2,\n",
       "  3197: 2,\n",
       "  3199: 4,\n",
       "  3200: 4,\n",
       "  3203: 2,\n",
       "  3205: 4,\n",
       "  3213: 6,\n",
       "  3215: 8,\n",
       "  3216: 4,\n",
       "  3219: 4,\n",
       "  3223: 9,\n",
       "  3225: 8,\n",
       "  3226: 6,\n",
       "  3230: 7,\n",
       "  3234: 8,\n",
       "  3238: 3,\n",
       "  3241: 4,\n",
       "  3242: 5,\n",
       "  3245: 6,\n",
       "  3248: 3,\n",
       "  3249: 3,\n",
       "  3251: 7,\n",
       "  3256: 5,\n",
       "  3257: 4,\n",
       "  3259: 7,\n",
       "  3260: 3,\n",
       "  3261: 6,\n",
       "  3262: 2,\n",
       "  3266: 3,\n",
       "  3280: 7,\n",
       "  3283: 3,\n",
       "  3285: 3,\n",
       "  3288: 4,\n",
       "  3289: 3,\n",
       "  3291: 2,\n",
       "  3292: 5,\n",
       "  3303: 11,\n",
       "  3307: 8,\n",
       "  3308: 7,\n",
       "  3309: 5,\n",
       "  3318: 3,\n",
       "  3319: 7,\n",
       "  3322: 5,\n",
       "  3325: 6,\n",
       "  3328: 4,\n",
       "  3329: 4,\n",
       "  3330: 4,\n",
       "  3337: 4,\n",
       "  3338: 4,\n",
       "  3340: 6,\n",
       "  3342: 3,\n",
       "  3347: 8,\n",
       "  3348: 6,\n",
       "  3354: 4,\n",
       "  3355: 2,\n",
       "  3358: 3,\n",
       "  3362: 8,\n",
       "  3364: 4,\n",
       "  3365: 7,\n",
       "  3367: 8,\n",
       "  3368: 6,\n",
       "  3370: 6,\n",
       "  3371: 5,\n",
       "  3384: 3,\n",
       "  3385: 8,\n",
       "  3388: 8,\n",
       "  3390: 8,\n",
       "  3395: 5,\n",
       "  3397: 6,\n",
       "  3398: 3,\n",
       "  3400: 4,\n",
       "  3404: 7,\n",
       "  3405: 8,\n",
       "  3417: 3,\n",
       "  3419: 3,\n",
       "  3422: 6,\n",
       "  3423: 5,\n",
       "  3427: 6,\n",
       "  3432: 5,\n",
       "  3433: 3,\n",
       "  3443: 8,\n",
       "  3449: 6,\n",
       "  3451: 5,\n",
       "  3452: 4,\n",
       "  3460: 9,\n",
       "  3461: 7,\n",
       "  3463: 3,\n",
       "  ...}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 유저 특징 정보 추출 \n",
    "profile_df = profile_df.set_index('profile_id') # profile_id를 인덱스로 지정\n",
    "user_features = profile_df[['age']].to_dict() # age를 dictionary로 변형\n",
    "print(\"user_id 3의 age 정보 :\", user_features['age'][3])\n",
    "display(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album_id 749의 genre_mid 정보 : 1\n"
     ]
    }
   ],
   "source": [
    "# 아이템 특징 정보 추출 \n",
    "meta_df = meta_df.set_index('album_id') # album_id를 인덱스로 지정\n",
    "\n",
    "# 범주형 데이터를 수치형 데이터로 변경 \n",
    "le = LabelEncoder() # 라벨인코더 생성\n",
    "meta_df['genre_mid'] = le.fit_transform(meta_df['genre_mid']) # fit transform\n",
    "item_features = meta_df[['genre_mid']].to_dict() # 장르를 딕셔너리 형태로 변경\n",
    "print(\"album_id 749의 genre_mid 정보 :\", item_features['genre_mid'][749]) # 시각적으로 한번 보는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 특징 정보의 속성을 저장 \n",
    "cfg.n_genres = meta_df['genre_mid'].nunique() # cfg 객체의 n_genres에 종류? 각각 중복되지 않는 요소 하나하나를 저장\n",
    "cfg.n_continuous_feats = 1 # 얘는 1로 지정, 밑에서 나올 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REzvMyWZYU1S"
   },
   "source": [
    "## NeuMF 구현\n",
    "\n",
    "\n",
    "### 모델 구현 \n",
    "- [Neural Collaborative Filtering(NCF)](https://arxiv.org/pdf/1708.05031.pdf) 논문의 NeuMF를 참고하여 side-information을 결합한 모델을 PyTorch로 구현\n",
    "    - continuous feature (age)와 categorical feature (genre_mid)를 같이 학습할 수 있도록 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=1tpajTLipLoFdvLICO-alAxeoKAE8-k61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hWOt2J5nYU1U"
   },
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \"\"\"Neural Matrix Factorization Model\n",
    "        참고 문헌 : https://arxiv.org/abs/1708.05031\n",
    "\n",
    "    예시 :\n",
    "        model = NeuMF(cfg) \n",
    "        output = model.forward(user_ids, item_ids, [feat0, feat1]) \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            cfg : config 파일로 네트워크 생성에 필요한 정보들을 담고 있음 \n",
    "        \"\"\"\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.n_users = cfg.n_users\n",
    "        self.n_items = cfg.n_items\n",
    "        self.emb_dim = cfg.emb_dim\n",
    "        self.layer_dim = cfg.layer_dim\n",
    "        self.n_continuous_feats = cfg.n_continuous_feats\n",
    "        self.n_genres = cfg.n_genres\n",
    "        self.dropout = cfg.dropout\n",
    "        self.build_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"Neural Matrix Factorization Model 생성\n",
    "            구현된 모습은 위의 그림을 참고 \n",
    "        \"\"\"\n",
    "        self.user_embedding_mf = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mf = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "        \n",
    "        self.user_embedding_mlp = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "                \n",
    "        self.genre_embeddig = nn.Embedding(num_embeddings=self.n_genres, embedding_dim=self.n_genres//2)\n",
    "        \n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim + self.n_genres//2 + self.n_continuous_feats, self.layer_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout), \n",
    "            nn.Linear(self.layer_dim, self.layer_dim//2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout)\n",
    "        )\n",
    "        self.affine_output = nn.Linear(self.layer_dim//2 + self.emb_dim, 1)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices, feats):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            user_indices : 유저의 인덱스 정보 \n",
    "                ex) tensor([ 3100,  3100,  ..., 14195, 14195])\n",
    "            item_indices : 아이템의 인덱스 정보\n",
    "                ex) tensor([   50,    65,   ..., 14960, 11527])\n",
    "            feats : 특징 정보 \n",
    "        Returns: \n",
    "            output : 유저-아이템 쌍에 대한 추천 결과 \n",
    "                ex) tensor([  9.4966,  22.0261, ..., -19.3535, -23.0212])\n",
    "        \"\"\"\n",
    "        user_embedding_mf = self.user_embedding_mf(user_indices)\n",
    "        item_embedding_mf = self.item_embedding_mf(item_indices)\n",
    "        mf_output = torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "        \n",
    "        user_embedding_mlp = self.user_embedding_mlp(user_indices)\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_indices)\n",
    "        genre_embedding_mlp = self.genre_embeddig(feats[1])\n",
    "        input_feature = torch.cat((user_embedding_mlp, item_embedding_mlp, genre_embedding_mlp, feats[0].unsqueeze(1)), -1)\n",
    "        mlp_output = self.mlp_layers(input_feature)\n",
    "        \n",
    "        output = torch.cat([mlp_output, mf_output], dim=-1)\n",
    "        output = self.affine_output(output).squeeze(-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE7ZtudJYU1V"
   },
   "source": [
    "### 학습 및 추론 코드 구현\n",
    "\n",
    "- 학습 : Negative sampling을 활용하여 Binary Classification 진행 \n",
    "    - history 에 있는 album_id는 positive label로 그렇지 않은 album_id는 nagative label로 활용  \n",
    "    - 단, 이때 모든 album_id를 negative label로 활용하는 것이 아닌 일부만 사용 (neg_ratio 값에 따라서 개수 조정)\n",
    "- 추론 : 일부 데이터에 대해 recall, ndcg, coverage 성능 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGTiX3PsYU1V"
   },
   "source": [
    "#### 학습 및 추론에 필요한 데이터 셋 생성 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_UIdataset(train, neg_ratio):\n",
    "    \"\"\" 유저별 학습에 필요한 딕셔너리 데이터 생성 \n",
    "    Args:\n",
    "        train : 유저-아이템의 상호작용을 담은 행렬 \n",
    "            ex) \n",
    "                array([[0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        ...,\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.]])\n",
    "        neg_ratio : negative sampling 활용할 비율 \n",
    "            ex) 3 (positive label 1개당 negative label 3개)\n",
    "    Returns: \n",
    "        UIdataset : 유저별 학습에 필요한 정보를 담은 딕셔너리 \n",
    "            ex) {'사용자 ID': [[positive 샘플, negative 샘플], ... , [1, 1, 1, ..., 0, 0]]}\n",
    "                >>> UIdataset[3]\n",
    "                    [array([   16,    17,    18, ...,  9586, 18991,  9442]),\n",
    "                    array([5, 5, 5, ..., 5, 5, 5]),\n",
    "                    array([4, 4, 4, ..., 5, 1, 1]),\n",
    "                    array([1., 1., 1., ..., 0., 0., 0.])]\n",
    "    \"\"\"\n",
    "    UIdataset = {}\n",
    "    for user_id, items_by_user in enumerate(train):\n",
    "        UIdataset[user_id] = []\n",
    "        # positive 샘플 계산 \n",
    "        pos_item_ids = np.where(items_by_user > 0.5)[0]\n",
    "        num_pos_samples = len(pos_item_ids)\n",
    "\n",
    "        # negative 샘플 계산 (random negative sampling) \n",
    "        num_neg_samples = neg_ratio * num_pos_samples\n",
    "        neg_items = np.where(items_by_user < 0.5)[0]\n",
    "        neg_item_ids = np.random.choice(neg_items, min(num_neg_samples, len(neg_items)), replace=False)\n",
    "        UIdataset[user_id].append(np.concatenate([pos_item_ids, neg_item_ids]))\n",
    "        \n",
    "        # feature 추출 \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "            features.append(user_features['age'][user_id])\n",
    "        UIdataset[user_id].append(np.array(features))\n",
    "        \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "            features.append(item_features['genre_mid'][item_id])\n",
    "        UIdataset[user_id].append(np.array(features))\n",
    "        \n",
    "        # label 저장  \n",
    "        pos_labels = np.ones(len(pos_item_ids))\n",
    "        neg_labels = np.zeros(len(neg_item_ids))\n",
    "        UIdataset[user_id].append(np.concatenate([pos_labels, neg_labels]))\n",
    "\n",
    "    return UIdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lak50\\Desktop\\baseline.ipynb 셀 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lak50/Desktop/baseline.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m UIdataset \u001b[39m=\u001b[39m make_UIdataset(train, neg_ratio\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mneg_ratio)\n",
      "\u001b[1;32mc:\\Users\\lak50\\Desktop\\baseline.ipynb 셀 28\u001b[0m in \u001b[0;36mmake_UIdataset\u001b[1;34m(train, neg_ratio)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lak50/Desktop/baseline.ipynb#X35sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m UIdataset[user_id] \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lak50/Desktop/baseline.ipynb#X35sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# positive 샘플 계산 \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lak50/Desktop/baseline.ipynb#X35sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m pos_item_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(items_by_user \u001b[39m>\u001b[39;49m \u001b[39m0.5\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lak50/Desktop/baseline.ipynb#X35sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m num_pos_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pos_item_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lak50/Desktop/baseline.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# negative 샘플 계산 (random negative sampling) \u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "UIdataset = make_UIdataset(train, neg_ratio=cfg.neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batchdata(user_indices, batch_idx, batch_size):\n",
    "    \"\"\" 배치 데이터로 변환 \n",
    "    Args:\n",
    "        user_indices : 전체 유저의 인덱스 정보 \n",
    "            ex) array([ 3100,  1800, 30098, ...,  2177, 11749, 20962])\n",
    "        batch_idx : 배치 인덱스 (몇번째 배치인지)\n",
    "            ex) 0 \n",
    "        batch_size : 배치 크기 \n",
    "            ex) 256 \n",
    "    Returns \n",
    "        batch_user_ids : 배치내의 유저 인덱스 정보 \n",
    "            ex) [22194, 22194, 22194, 22194, 22194, ...]\n",
    "        batch_item_ids : 배치내의 아이템 인덱스 정보 \n",
    "            ex) [36, 407, 612, 801, 1404, ...]\n",
    "        batch_feat0 : 배치내의 유저-아이템 인덱스 정보에 해당하는 feature0 정보 \n",
    "            ex) [6, 6, 6, 6, 6, ...]\n",
    "        batch_feat1 : 배치내의 유저-아이템 인덱스 정보에 해당하는 feature1 정보 \n",
    "            ex) [4,  4,  4, 23,  4, ...]\n",
    "        batch_labels : 배치내의 유저-아이템 인덱스 정보에 해당하는 label 정보 \n",
    "            ex) [1.0, 1.0, 1.0, 1.0, 1.0, ...]\n",
    "    \"\"\"\n",
    "    batch_user_indices = user_indices[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "    batch_user_ids = []\n",
    "    batch_item_ids = []\n",
    "    batch_feat0 = []\n",
    "    batch_feat1 = []\n",
    "    batch_labels = []\n",
    "    for user_id in batch_user_indices:\n",
    "        item_ids = UIdataset[user_id][0]\n",
    "        feat0 = UIdataset[user_id][1]\n",
    "        feat1 = UIdataset[user_id][2]\n",
    "        labels = UIdataset[user_id][3]\n",
    "        user_ids = np.full(len(item_ids), user_id)\n",
    "        batch_user_ids.extend(user_ids.tolist())\n",
    "        batch_item_ids.extend(item_ids.tolist())\n",
    "        batch_feat0.extend(feat0.tolist())\n",
    "        batch_feat1.extend(feat1.tolist())\n",
    "        batch_labels.extend(labels.tolist())\n",
    "    return batch_user_ids, batch_item_ids, batch_feat0, batch_feat1, batch_labels\n",
    "\n",
    "def update_avg(curr_avg, val, idx):\n",
    "    \"\"\" 현재 epoch 까지의 평균 값을 계산 \n",
    "    \"\"\"\n",
    "    return (curr_avg * idx + val) / (idx + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 및 검증 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dS1EPRpeYU1W"
   },
   "outputs": [],
   "source": [
    "def train_epoch(cfg, model, optimizer, criterion): \n",
    "    model.train()\n",
    "    curr_loss_avg = 0.0\n",
    "\n",
    "    user_indices = np.arange(cfg.n_users)\n",
    "    np.random.RandomState(cfg.epoch).shuffle(user_indices)\n",
    "    batch_num = int(len(user_indices) / cfg.batch_size) + 1\n",
    "    bar = tqdm(range(batch_num), leave=False)\n",
    "    for step, batch_idx in enumerate(bar):\n",
    "        user_ids, item_ids, feat0, feat1, labels = make_batchdata(user_indices, batch_idx, cfg.batch_size)\n",
    "        # 배치 사용자 단위로 학습\n",
    "        user_ids = torch.LongTensor(user_ids).to(cfg.device)\n",
    "        item_ids = torch.LongTensor(item_ids).to(cfg.device)\n",
    "        feat0 = torch.FloatTensor(feat0).to(cfg.device)\n",
    "        feat1 = torch.LongTensor(feat1).to(cfg.device)\n",
    "        labels = torch.FloatTensor(labels).to(cfg.device)\n",
    "        labels = labels.view(-1, 1)\n",
    "\n",
    "        # grad 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 forward\n",
    "        output = model.forward(user_ids, item_ids, [feat0, feat1])\n",
    "        output = output.view(-1, 1)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 최적화\n",
    "        optimizer.step()    \n",
    "        if torch.isnan(loss):\n",
    "            print('Loss NAN. Train finish.')\n",
    "            break\n",
    "        curr_loss_avg = update_avg(curr_loss_avg, loss, step)\n",
    "        \n",
    "        msg = f\"epoch: {cfg.epoch}, \"\n",
    "        msg += f\"loss: {curr_loss_avg.item():.5f}, \"\n",
    "        msg += f\"lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        bar.set_description(msg)\n",
    "    rets = {'losses': np.around(curr_loss_avg.item(), 5)}\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(actual, predicted, k = 25):\n",
    "    \"\"\" label과 prediction 사이의 recall 평가 함수 \n",
    "    Args:\n",
    "        actual : 실제로 본 상품 리스트\n",
    "        pred : 예측한 상품 리스트\n",
    "        k : 상위 몇개의 데이터를 볼지 (ex : k=5 상위 5개의 상품만 봄)\n",
    "    Returns: \n",
    "        recall_k : recall@k \n",
    "    \"\"\" \n",
    "    set_actual = set(actual)\n",
    "    recall_k = len(set_actual & set(predicted[:k])) / min(k, len(set_actual))\n",
    "    return recall_k\n",
    "\n",
    "def unique(sequence):\n",
    "    # preserves order\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def ndcgk(actual, predicted, k = 25):\n",
    "    set_actual = set(actual)\n",
    "    idcg = sum([1.0 / np.log(i + 2) for i in range(min(k, len(set_actual)))])\n",
    "    dcg = 0.0\n",
    "    unique_predicted = unique(predicted[:k])\n",
    "    for i, r in enumerate(unique_predicted):\n",
    "        if r in set_actual:\n",
    "            dcg += 1.0 / np.log(i + 2)\n",
    "    ndcg_k = dcg / idcg\n",
    "    return ndcg_k\n",
    "\n",
    "def evaluation(gt, pred):\n",
    "    \"\"\" label과 prediction 사이의 recall, coverage, competition metric 평가 함수 \n",
    "    Args:\n",
    "        gt : 데이터 프레임 형태의 정답 데이터 \n",
    "        pred : 데이터 프레임 형태의 예측 데이터 \n",
    "    Returns: \n",
    "        rets : recall, ndcg, coverage, competition metric 결과 \n",
    "            ex) {'recall': 0.123024, 'ndcg': 056809, 'coverage': 0.017455, 'score': 0.106470}\n",
    "    \"\"\"    \n",
    "    gt = gt.groupby('profile_id')['album_id'].unique().to_frame().reset_index()\n",
    "    gt.columns = ['profile_id', 'actual_list']\n",
    "\n",
    "    evaluated_data = pd.merge(pred, gt, how = 'left', on = 'profile_id')\n",
    "\n",
    "    evaluated_data['Recall@25'] = evaluated_data.apply(lambda x: recallk(x.actual_list, x.predicted_list), axis=1)\n",
    "    evaluated_data['NDCG@25'] = evaluated_data.apply(lambda x: ndcgk(x.actual_list, x.predicted_list), axis=1)\n",
    "\n",
    "    recall = evaluated_data['Recall@25'].mean()\n",
    "    ndcg = evaluated_data['NDCG@25'] .mean()\n",
    "    coverage = (evaluated_data['predicted_list'].apply(lambda x: x[:cfg.top_k]).explode().nunique())/meta_df.index.nunique()\n",
    "\n",
    "    score = 0.75*recall + 0.25*ndcg\n",
    "    rets = {\"recall\" :recall, \n",
    "            \"ndcg\" :ndcg, \n",
    "            \"coverage\" :coverage, \n",
    "            \"score\" :score}\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FQgUJbTOYU1X"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(cfg, model, data, mode='valid'):\n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    query_user_ids = data['profile_id'].unique() # 추론할 모든 user array 집합\n",
    "    full_item_ids = np.array([c for c in range(cfg.n_items)]) # 추론할 모든 item array 집합 \n",
    "    full_item_ids_feat1 = [item_features['genre_mid'][c] for c in full_item_ids]\n",
    "    for user_id in query_user_ids:\n",
    "        with torch.no_grad():\n",
    "            user_ids = np.full(cfg.n_items, user_id)\n",
    "            \n",
    "            user_ids = torch.LongTensor(user_ids).to(cfg.device)\n",
    "            item_ids = torch.LongTensor(full_item_ids).to(cfg.device)\n",
    "            \n",
    "            feat0 = np.full(cfg.n_items, user_features['age'][user_id])\n",
    "            feat0 = torch.FloatTensor(feat0).to(cfg.device)\n",
    "            feat1 = torch.LongTensor(full_item_ids_feat1).to(cfg.device)\n",
    "            \n",
    "            eval_output = model.forward(user_ids, item_ids, [feat0, feat1]).detach().cpu().numpy()\n",
    "            pred_u_score = eval_output.reshape(-1)   \n",
    "        \n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = full_item_ids[pred_u_idx]\n",
    "        pred_list.append(list(pred_u[:cfg.top_k]))\n",
    "        \n",
    "    pred = pd.DataFrame()\n",
    "    pred['profile_id'] = query_user_ids\n",
    "    pred['predicted_list'] = pred_list\n",
    "    \n",
    "    # 모델 성능 확인 \n",
    "    if mode == 'valid':\n",
    "        rets = evaluation(data, pred)\n",
    "        return rets, pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-D5bJx7YU1X"
   },
   "source": [
    "### 하이퍼파라미터 설정 & 최적화 기법 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정 \n",
    "cfg.batch_size = 64\n",
    "cfg.emb_dim = 256\n",
    "cfg.layer_dim = 256\n",
    "cfg.dropout = 0.05\n",
    "cfg.epochs = 25\n",
    "cfg.learning_rate = 0.0025\n",
    "cfg.reg_lambda = 0\n",
    "cfg.check_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 생성 및 optimizer, loss 함수 설정 \n",
    "model = NeuMF(cfg).to(cfg.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.reg_lambda)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eze0e7vtYU1Y"
   },
   "source": [
    "### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cvhj3hD7YU1Y",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c9b2b935c74444b165fe0e5e9f3d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\aivle\\2022_uplus_ai_ground\\code\\baseline.ipynb 셀 34\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cfg\u001b[39m.\u001b[39mepochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cfg\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     train_results \u001b[39m=\u001b[39m train_epoch(cfg, model, optimizer, criterion)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# cfg.check_epoch 번의 epoch 마다 성능 확인 \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m cfg\u001b[39m.\u001b[39mcheck_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \n",
      "\u001b[1;32me:\\aivle\\2022_uplus_ai_ground\\code\\baseline.ipynb 셀 34\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(cfg, model, optimizer, criterion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# 모델 forward\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(user_ids, item_ids, [feat0, feat1])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n",
      "\u001b[1;32me:\\aivle\\2022_uplus_ai_ground\\code\\baseline.ipynb 셀 34\u001b[0m in \u001b[0;36mNeuMF.forward\u001b[1;34m(self, user_indices, item_indices, feats)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, user_indices, item_indices, feats):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m        user_indices : 유저의 인덱스 정보 \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m            ex) tensor([  9.4966,  22.0261, ..., -19.3535, -23.0212])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     user_embedding_mf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_embedding_mf(user_indices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     item_embedding_mf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_embedding_mf(item_indices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/aivle/2022_uplus_ai_ground/code/baseline.ipynb#X45sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     mf_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(user_embedding_mf, item_embedding_mf)\n",
      "File \u001b[1;32mc:\\Users\\lak50\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lak50\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\lak50\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_logs = defaultdict(list)\n",
    "best_scores  = 0\n",
    "for epoch in range(cfg.epochs+1):\n",
    "    cfg.epoch = epoch\n",
    "    train_results = train_epoch(cfg, model, optimizer, criterion)\n",
    "    \n",
    "    # cfg.check_epoch 번의 epoch 마다 성능 확인 \n",
    "    if epoch % cfg.check_epoch == 0: \n",
    "        valid_results, _ = valid_epoch(cfg, model, valid)\n",
    "\n",
    "        logs = {\n",
    "            'Train Loss': train_results['losses'],\n",
    "            f'Valid Recall@{cfg.top_k}': valid_results['recall'],\n",
    "            f'Valid NDCG@{cfg.top_k}': valid_results['ndcg'],\n",
    "            'Valid Coverage': valid_results['coverage'],\n",
    "            'Valid Score': valid_results['score'],\n",
    "            }\n",
    "\n",
    "        # 검증 성능 확인 \n",
    "        for key, value in logs.items():\n",
    "            total_logs[key].append(value)\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(\"Epoch\", end=\",\")\n",
    "            print(\",\".join(logs.keys()))\n",
    "\n",
    "        print(f\"{epoch:02d}  \", end=\"\")\n",
    "        print(\"  \".join([f\"{v:0.6f}\" for v in logs.values()]))\n",
    "        \n",
    "        # 가장 성능이 좋은 가중치 파일을 저장 \n",
    "        if best_scores <= valid_results['score']: \n",
    "            best_scores = valid_results['score']\n",
    "            torch.save(model.state_dict(), os.path.join(saved_path, 'model(best_scores).pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNVpn37RYU1Z"
   },
   "source": [
    "### 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = pd.DataFrame({'Train loss': total_logs['Train Loss']})\n",
    "train_scores['Epoch'] = range(0, cfg.epochs+1, cfg.check_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(train_scores, aes(x='Epoch', y='Train loss'))\n",
    "        + geom_line(color='black') # line plot\n",
    "        + labs(x='Epoch', y='Train Loss')\n",
    "        + theme_light()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores = pd.DataFrame(np.hstack([(range(0, cfg.epochs+1, cfg.check_epoch), total_logs[score], [score for i in range(0, cfg.epochs+1, cfg.check_epoch)]) for score in ['Valid Recall@25', 'Valid NDCG@25', 'Valid Coverage', 'Valid Score']])).T\n",
    "valid_scores.columns = ['Epoch', 'Score', 'Metric']\n",
    "valid_scores['Epoch'] = valid_scores['Epoch'].astype(int)\n",
    "valid_scores['Score'] = valid_scores['Score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "ggplot(valid_scores)  # here\n",
    "    + aes(\"Epoch\", \"Score\", color='Metric', group='Metric')\n",
    "    + geom_line()\n",
    "    + scale_y_continuous(breaks=[0.1*c for c in range(1, 10, 1)])\n",
    "    + theme_light()\n",
    "    + labs(x='Epoch', y='Valid Metric')\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07lHW5CAYU1F"
   },
   "source": [
    "## 제출 \n",
    "### 모든 유저에 대해 추천 결과 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(saved_path, 'model(best_scores).pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = os.path.join(data_path, 'sample_submission.csv')\n",
    "submission = pd.read_csv(submission_path)\n",
    "submission = valid_epoch(cfg, model, submission, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(output_path, 'submission.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "기본과제-2_NCF+AutoRec (answer).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "964f46f3973c3d99dc6929d5bd1748275317def86f082ea2874e0d5a9cf6d261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOKnNSACYU0-"
   },
   "source": [
    "# LGU+ 경진대회 - 베이스라인  \n",
    "- [Neural Collaborative Filtering(NCF)](https://arxiv.org/pdf/1708.05031.pdf) 논문의 NeuMF를 참고하여 side-information을 결합한 모델을 PyTorch로 구현\n",
    "- 구현된 모델의 검증 데이터셋과 리더보드의 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차 \n",
    "- 데이터 전처리 \n",
    "    - 기본 설정\n",
    "    - 데이터 불러오기 \n",
    "    - 학습 및 검증 데이터 생성 \n",
    "- NeuMF 구현    \n",
    "    - 모델 구현 \n",
    "    - 학습 및 추론 코드 구현\n",
    "- 모델 학습 \n",
    "    - 하이퍼 파라미터 설정 & 최적화 기법 설정\n",
    "    - 모델 학습 \n",
    "    - 학습 과정 시각화 \n",
    "- 제출 \n",
    "    - 모든 유저에 대해 추천 결과 생성\n",
    "    - 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07lHW5CAYU1F"
   },
   "source": [
    "## 데이터 전처리\n",
    "### 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색용 : forsearch\n",
    "# 1. 패키지 로드 : package (# 패키지 로드)\n",
    "# 2. cfg : 클래스cfg (# cfg 하이퍼 파라미터)\n",
    "# 3. seed_everything : fixseed (# 시드 고정)\n",
    "# 4. 경로 : localpath (# 경로 설정)\n",
    "# 5. 불러온 데이터 : originaldata (# 데이터 불러오기)\n",
    "# 6. 중복제거 후 rating열 추가 : dropduplicate (# 데이터 전처리 (중복제거))\n",
    "# 7. train_test_split : traintestsplist (# 학습 및 검증 데이터 분리)\n",
    "# 8. matrix를 만들고 그걸 rating에 대응 : makematrix (# Matrix 형태로 변환)\n",
    "# 9. profile_id 대응 나이 추출 : userfeaturesextraction (# 유저 특징 정보 추출)\n",
    "# 10. album_id 대응 장르 mid 추출 : itemfeaturesextraction (# 아이템 특징 정보 추출)\n",
    "# 11. genre_mid 속성과 continues_feats 저장 : savegenremidnunique (# 추출한 특징 정보의 속성을 저장)\n",
    "# 12. 모델 구현 : modeling (# 모델 구현)\n",
    "# 13. dateset을 만듬 : makeUIdateset (# 학습 및 추론 코드 구현)\n",
    "# 14. UIdataset을 만듬 : realmakeUIdataset\n",
    "# 15. batchdata를 만듬 : makebatchdata\n",
    "# 16. 여기서 학습을 시키는 것 같음 : Learningandverification\n",
    "# 17. 검증, 평가 : Learningandverification2\n",
    "# 18. 이해 중 : Learningandverification3\n",
    "# 19. 하이퍼 파라미터 설정 : 하이퍼파라미터\n",
    "# 20. model 생성 및 optimizer, loss 함수 설정  : settingmodeloptimizerloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iHstgRfiYU1G"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# 패키지 로드\n",
    "# package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os, random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import plotnine\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# 하이퍼 파라미터 \n",
    "# 클래스cfg\n",
    "class cfg: \n",
    "    gpu_idx = 0\n",
    "    device = torch.device(\"cuda:{}\".format(gpu_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "    top_k = 25\n",
    "    seed = 42\n",
    "    neg_ratio = 100\n",
    "    test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5GrHkU7AYU1I"
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "# 시드 고정 \n",
    "# fixseed\n",
    "def seed_everything(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# 경로 설정\n",
    "# localpath\n",
    "data_path = '../data'\n",
    "saved_path = './saved'\n",
    "output_path = './submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huw6BwpTYU1J"
   },
   "source": [
    "### 데이터 불러오기\n",
    "- history_data : 시청 시작 데이터\n",
    "- profile_data : 프로필 정보 \n",
    "- meta_data : 콘텐츠 일반 메타 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A23qw5LXYU1M"
   },
   "outputs": [],
   "source": [
    "# 5\n",
    "# 데이터 불러오기 \n",
    "# originaldata\n",
    "history_df = pd.read_csv(os.path.join(data_path, 'history_data.csv'), encoding='utf-8')\n",
    "profile_df = pd.read_csv(os.path.join(data_path, 'profile_data.csv'), encoding='utf-8')\n",
    "meta_df = pd.read_csv(os.path.join(data_path, 'meta_data.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRnh1wEiYU1M"
   },
   "source": [
    "### 학습 및 검증 데이터 생성 \n",
    "- train : 시청 이력의 80%를 사용 \n",
    "- valid : 시청 이력의 20%를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# 데이터 전처리 (중복제거) \n",
    "# dropduplicate\n",
    "# 참고 : drop_duplicates의 subset을 무엇으로 구성하냐에 따라서 제거되는 항목들이 다름 \n",
    "# ex) 'profile_id', 'album_id' : 중복된 시청이력 모두 제거 / 'profile_id', 'album_id', 'log_time' : 같은 시간에 시청한 이력만 제거 \n",
    "data = history_df[['profile_id', 'log_time', 'album_id']].drop_duplicates(subset=['profile_id', 'album_id', 'log_time']).sort_values(by = ['profile_id', 'log_time']).reset_index(drop = True)\n",
    "data['rating'] = 1\n",
    "\n",
    "cfg.n_users = data.profile_id.max()+1 # cfg n_users에 profile_id 최댓값+1 값을 유저수로 추가\n",
    "cfg.n_items = data.album_id.max()+1 # item 개수는 album_id 최댓값+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: (719401, 4)\n",
      "검증 데이터 크기: (179851, 4)\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "# 학습 및 검증 데이터 분리\n",
    "# traintestsplist\n",
    "train, valid = train_test_split(\n",
    "    data, test_size=cfg.test_size, random_state=cfg.seed,\n",
    ") # test_size = 0.2, seed = 42\n",
    "print('학습 데이터 크기:', train.shape)\n",
    "print('검증 데이터 크기:', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35fe58f2eb74c05a278de035c079301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "train 형태: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "# Matrix 형태로 변환 \n",
    "# makematrix\n",
    "train = train.to_numpy() # train data를 넘파이 형태로 변환\n",
    "matrix = sparse.lil_matrix((cfg.n_users, cfg.n_items)) # lil_matrix((M, N), [dtype]) : dtype은 선택, 모양이 m, n인 **빈 행렬** 구성\n",
    "'''\n",
    "2. PreliminariesPermalink의\n",
    "    Learning from implicit data\n",
    "    \n",
    "    -- 여기서의 0은 비선호를 의미하지는 않음, 해당 아이템을 모를때에도 0으로 표기됨\n",
    "'''\n",
    "for (p, _, i, r) in tqdm(train): # p = profile_id 첫번째 행, i = album_id 첫번째 행, r = rating 첫번째 행\n",
    "    matrix[p, i] = r # <-- 그냥 값을 뽑아서 매트릭스를 만들고 그 값을 1에 대응시키는거 같음\n",
    "train = sparse.csr_matrix(matrix) # 매트릭스를 다시 희소 행렬로 변환 --> 0을 제외한 데이터를 저장한다, \n",
    "print('---'*30)\n",
    "train = train.toarray() # 다시 array로 변환\n",
    "print(\"train 형태: \\n\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id 3의 age 정보 : 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: 5, 5: 5, 7: 9, 12: 6, 16: 12}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9\n",
    "# 유저 특징 정보 추출 \n",
    "# userfeaturesextraction\n",
    "from itertools import islice\n",
    "\n",
    "profile_df = pd.read_csv(os.path.join(data_path, 'profile_data.csv'), encoding='utf-8')\n",
    "\n",
    "profile_df = profile_df.set_index('profile_id') # profile_id를 인덱스로 지정\n",
    "user_features = profile_df[['age']].to_dict() # profile_df의 age를 dictionary로 변형\n",
    "print(\"user_id 3의 age 정보 :\", user_features['age'][3]) # user_id와 age를 엮어주는거 같음, \n",
    "# user_features라는 딕셔너리 안의 key값은 age, age키의 값이 profile_id를 key로 가지고 나이를 값으로 가지는 딕셔너리가 됨 (user_features는 이중 딕셔너리)\n",
    "\n",
    "dict(islice(user_features['age'].items(), 5)) # 딕셔너리 인덱스 5까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album_id 749의 genre_mid 정보 : 1\n"
     ]
    }
   ],
   "source": [
    "# 10\n",
    "# 아이템 특징 정보 추출 \n",
    "# itemfeaturesextraction\n",
    "meta_df = pd.read_csv(os.path.join(data_path, 'meta_data.csv'), encoding='utf-8')\n",
    "\n",
    "meta_df = meta_df.set_index('album_id')\n",
    "\n",
    "# 범주형 데이터를 수치형 데이터로 변경 \n",
    "le = LabelEncoder()\n",
    "meta_df['genre_mid'] = le.fit_transform(meta_df['genre_mid'])\n",
    "item_features = meta_df[['genre_mid']].to_dict()\n",
    "print(\"album_id 749의 genre_mid 정보 :\", item_features['genre_mid'][749])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# 추출한 특징 정보의 속성을 저장\n",
    "# savegenremidnunique\n",
    "cfg.n_genres = meta_df['genre_mid'].nunique()\n",
    "cfg.n_continuous_feats = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "25917\n",
      "------------------------------------------------------------\n",
      "1\n",
      "0\n",
      "25917\n",
      "------------------------------------------------------------\n",
      "2\n",
      "0\n",
      "25917\n",
      "------------------------------------------------------------\n",
      "3\n",
      "13\n",
      "25904\n",
      "------------------------------------------------------------\n",
      "4\n",
      "0\n",
      "25917\n",
      "------------------------------------------------------------\n",
      "5\n",
      "233\n",
      "25684\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 13번 셀 positive, negative 참고 (ReferencePN)\n",
    "# \n",
    "count = 0 # 3번 5번 이렇게 profile_id 데이터 값이 있어서 이것들만 나오는거 같음\n",
    "\n",
    "for user_id, items_by_user in enumerate(train):\n",
    "    print(count)\n",
    "    pos_item_ids = np.where(items_by_user > 0.5)[0]\n",
    "    num_pos_samples = len(pos_item_ids)\n",
    "    print(len(pos_item_ids))\n",
    "    num_neg_samples = cfg.neg_ratio * num_pos_samples\n",
    "    neg_items = np.where(items_by_user < 0.5)[0]\n",
    "    print(len(neg_items))\n",
    "    print('--'*30)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REzvMyWZYU1S"
   },
   "source": [
    "## NeuMF 구현\n",
    "\n",
    "\n",
    "### 모델 구현 \n",
    "- [Neural Collaborative Filtering(NCF)](https://arxiv.org/pdf/1708.05031.pdf) 논문의 NeuMF를 참고하여 side-information을 결합한 모델을 PyTorch로 구현\n",
    "    - continuous feature (age)와 categorical feature (genre_mid)를 같이 학습할 수 있도록 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=1tpajTLipLoFdvLICO-alAxeoKAE8-k61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hWOt2J5nYU1U"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:82\u001b[1;36m\u001b[0m\n\u001b[1;33m    def _init_weights(self, module):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "# modeling\n",
    "\n",
    "class NeuMF(nn.Module): # 파이토치 모델링의 기본 골자\n",
    "    \"\"\"Neural Matrix Factorization Model\n",
    "        참고 문헌 : https://arxiv.org/abs/1708.05031\n",
    "\n",
    "    예시 :\n",
    "        model = NeuMF(cfg) \n",
    "        output = model.forward(user_ids, item_ids, [feat0, feat1]) \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg): # 초기화 함수\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            cfg : config 파일로 네트워크 생성에 필요한 정보들을 담고 있음 \n",
    "        \"\"\"\n",
    "        super(NeuMF, self).__init__()\n",
    "        # __init()__에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function(nn.functional.relu, nn.functional.sigmoid)등을 정의한다. \n",
    "        # 여기서는 모델명 = NeuMF\n",
    "        \n",
    "# n_users = data.profile_id.max()+1, \n",
    "# n_items = data.album_id.max()+1\n",
    "# emb_dim = 256\n",
    "# layer_dim = 256\n",
    "# n_continuous_feats = 1\n",
    "# n_genres = meta_df['genre_mid'].nunique() = 29, //2하면 14\n",
    "# dropout = 0.05\n",
    "        \n",
    "        self.n_users = cfg.n_users\n",
    "        self.n_items = cfg.n_items\n",
    "        self.emb_dim = cfg.emb_dim\n",
    "        self.layer_dim = cfg.layer_dim\n",
    "        self.n_continuous_feats = cfg.n_continuous_feats\n",
    "        self.n_genres = cfg.n_genres\n",
    "        self.dropout = cfg.dropout\n",
    "        self.build_graph() # forward 이전 input 코딩이 너무 크므로 함수 build_graph로 대체하여 집어넣은 것 같음\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"Neural Matrix Factorization Model 생성\n",
    "            구현된 모습은 위의 그림을 참고 \n",
    "        \"\"\"\n",
    "        # nn.Embedding 참고\n",
    "''' \n",
    "num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기입니다. <--\n",
    "embedding_dim : 임베딩 할 벡터의 차원입니다. 사용자가 정해주는 하이퍼 파라미터입니다. <--\n",
    "padding_idx : 선택적으로 사용하는 인자입니다. 패딩을 위한 토큰의 인덱스를 알려줍니다.\n",
    "'''\n",
    "        self.user_embedding_mf = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim) # n_users = data.profile_id.max()+1, emb_dim = 256\n",
    "        self.item_embedding_mf = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "        # ex) embedding_dim이 3, num_embeddings=8이면 // num_embeddings 행 개수, embedding_dim이 열 개수\n",
    "        \n",
    "        # tensor([[-0.1778, -1.9974, -1.2478],\n",
    "        # [ 0.0000,  0.0000,  0.0000],\n",
    "        # [ 1.0921,  0.0416, -0.7896],\n",
    "        # [ 0.0960, -0.6029,  0.3721],\n",
    "        # [ 0.2780, -0.4300, -1.9770],\n",
    "        # [ 0.0727,  0.5782, -3.2617],\n",
    "        # [-0.0173, -0.7092,  0.9121],\n",
    "        # [-0.4817, -1.1222,  2.2774]], requires_grad=True)\n",
    "        \n",
    "        self.user_embedding_mlp = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim) # 여기까지 4줄은 다 똑같음\n",
    "                \n",
    "        self.genre_embeddig = nn.Embedding(num_embeddings=self.n_genres, embedding_dim=self.n_genres//2) # 장르 임베딩 하이퍼 파라미터는 n_genres = meta_df['genre_mid'].nunique()\n",
    "                                                                                                        # 임베딩 차원이 장르/2라는 특이한 점이 있음\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim + self.n_genres//2 + self.n_continuous_feats, self.layer_dim), # nn.Linear참고\n",
    "            # nn.Linear의 형태는 nn.Linear(input_dim,output_dim)이다\n",
    "            # 여기서 input은 256*2 + 29//2 + 1, output은 256\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout), # dropout = 0.05\n",
    "            nn.Linear(self.layer_dim, self.layer_dim//2), # layer_dim = 256\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout)\n",
    "        )\n",
    "        self.affine_output = nn.Linear(self.layer_dim//2 + self.emb_dim, 1) # affine output 밑에 나올지도 모름, input --> 256//2 + 256, output --> 1\n",
    "        self.apply(self._init_weights) # _init_weights는 바로 아래 함수, 전체에 이 _init_weights를 적용함 <-- 가중치를 초기화하는 함수\n",
    "        \n",
    "    ################################################################################################################################\n",
    "    #                                                        _init_weights_                                                        #\n",
    "    ################################################################################################################################\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding): # isinstance(확인하고자 하는 데이터 값, 확인하고자 하는 데이터 타입)\n",
    "            # 같으면 True, 다르면 False 반환\n",
    "            normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "\n",
    "            '''\n",
    "            torch.nn.init.normal_(tensor, mean=0.0, std=1.0)[SOURCE]\n",
    "            \n",
    "                Fills the input Tensor with values drawn from the normal distribution \\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std \n",
    "                (translate : 정규분포에서 가져온 값으로 입력 Tensor를 채웁니다)\n",
    "                \n",
    "            Parameters:\n",
    "                tensor (Tensor) an n-dimensional torch.Tensor (Tensor의 차원)\n",
    "                mean (float) the mean of the normal distribution (정규분포의 평균)\n",
    "                std (float) the standard deviation of the normal distribution (정규분포의 표준편차)\n",
    "            \n",
    "            '''\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices, feats): # forward()에서는 모델에서 실행되어야하는 계산을 정의한다\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            user_indices : 유저의 인덱스 정보 \n",
    "                ex) tensor([ 3100,  3100,  ..., 14195, 14195])\n",
    "            item_indices : 아이템의 인덱스 정보\n",
    "                ex) tensor([   50,    65,   ..., 14960, 11527])\n",
    "            feats : 특징 정보 \n",
    "        Returns: \n",
    "            output : 유저-아이템 쌍에 대한 추천 결과 \n",
    "                ex) tensor([  9.4966,  22.0261, ..., -19.3535, -23.0212])\n",
    "        \"\"\"\n",
    "        user_embedding_mf = self.user_embedding_mf(user_indices) # line 49\n",
    "        item_embedding_mf = self.item_embedding_mf(item_indices) # line 50\n",
    "        mf_output = torch.mul(user_embedding_mf, item_embedding_mf) # mul = 곱, tensor로 출력\n",
    "        \n",
    "        user_embedding_mlp = self.user_embedding_mlp(user_indices) # line 62\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_indices) # line 63\n",
    "        genre_embedding_mlp = self.genre_embeddig(feats[1]) # feats[1]은 아래 16번 셀의 24번 줄에서 나옴, feats로 들어오는 리스트의 1번째 인덱스 값을 넣는다는 뜻\n",
    "        input_feature = torch.cat((user_embedding_mlp, item_embedding_mlp, genre_embedding_mlp, feats[0].unsqueeze(1)), -1)\n",
    "        # torch.cat = 주어진 차원에서 주어진 텐서 시퀀스를 연결 합니다. 모든 텐서는 모양이 같거나(연결 차원 제외) 비어 있어야 합니다. <-- 그냥 concat인듯\n",
    "        \n",
    "        # unsqueeze는 차원을 생성해주는 함수, 1 이니 1 인덱스에 생성될 것임 (ex) x.unsqueeze(dim=1) #[3, 20, 128] -> [3, 1, 20, 128]) \n",
    "        \n",
    "        mlp_output = self.mlp_layers(input_feature) # mlp_layers = 아까 쌓았던 Linear, Relu 이거 모델\n",
    "        # 모델에 input으로 mlp들과 feats[0]을 곱한걸 넣어줌 = 그게 mlp_output\n",
    "        \n",
    "        output = torch.cat([mlp_output, mf_output], dim=-1) # 가장 높은차원부터면 오름차순, 가장 낮은차원부터는 내림차순\n",
    "        # dim = -1은 가장 낮은 차원에서 + 1, 1차원이 가장 낮을테니 아마 2차원\n",
    "        # mf는 profile, album임, mlp도 아마 그런거같음\n",
    "        \n",
    "        output = self.affine_output(output).squeeze(-1) # line 76\n",
    "        # 가장 낮은 차원에서 하나 위, 아마 2차원 삭제\n",
    "        # 해석해보면 리니어 층을 통과 시킨 뒤 차원 하나 삭제?\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE7ZtudJYU1V"
   },
   "source": [
    "### 학습 및 추론 코드 구현\n",
    "\n",
    "- 학습 : Negative sampling을 활용하여 Binary Classification 진행 \n",
    "    - history 에 있는 album_id는 positive label로 그렇지 않은 album_id는 nagative label로 활용  \n",
    "    - 단, 이때 모든 album_id를 negative label로 활용하는 것이 아닌 일부만 사용 (neg_ratio 값에 따라서 개수 조정)\n",
    "- 추론 : 일부 데이터에 대해 recall, ndcg, coverage 성능 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGTiX3PsYU1V"
   },
   "source": [
    "#### 학습 및 추론에 필요한 데이터 셋 생성 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "# makeUIdateset\n",
    "\n",
    "def make_UIdataset(train, neg_ratio):\n",
    "    \"\"\" 유저별 학습에 필요한 딕셔너리 데이터 생성 \n",
    "    Args:\n",
    "        train : 유저-아이템의 상호작용을 담은 행렬 \n",
    "            ex) \n",
    "                array([[0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        ...,\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.]])\n",
    "        neg_ratio : negative sampling 활용할 비율, negative sampling은 전체 데이터를 활용하는게 아니라 중심 데이터와 상관없는 것 중 몇개만 추출하여 사용, \n",
    "                                                아마 주변 데이터를 활용하는건 반대로 positive 같음\n",
    "            ex) 3 (positive label 1개당 negative label 3개)\n",
    "    Returns: \n",
    "        UIdataset : 유저별 학습에 필요한 정보를 담은 딕셔너리 \n",
    "            ex) {'사용자 ID': [[positive 샘플, negative 샘플], ... , [1, 1, 1, ..., 0, 0]]}\n",
    "                >>> UIdataset[3]\n",
    "                    [array([   16,    17,    18, ...,  9586, 18991,  9442]),\n",
    "                    array([5, 5, 5, ..., 5, 5, 5]),\n",
    "                    array([4, 4, 4, ..., 5, 1, 1]),\n",
    "                    array([1., 1., 1., ..., 0., 0., 0.])]\n",
    "    \"\"\"\n",
    "    UIdataset = {}\n",
    "    for user_id, items_by_user in enumerate(train): # user_id = 인덱스, items_by_user = 인덱스에 해당하는 데이터, 참고 검색 --> ReferencePN\n",
    "        UIdataset[user_id] = [] # UIdataset의 user_id들에게 리스트 할당\n",
    "        # positive 샘플 계산 \n",
    "        pos_item_ids = np.where(items_by_user > 0.5)[0] # items_by_user에서 0.5 초과인 값의 profile_id 추적 --> list형태로 나옴\n",
    "        num_pos_samples = len(pos_item_ids) # 0.5이상인 전체 값의 개수\n",
    "\n",
    "        # negative 샘플 계산 (random negative sampling) \n",
    "        num_neg_samples = neg_ratio * num_pos_samples # neg_ratio = negative sampling 활용을 얼마나 할지 결정하지 위해 곱해줌\n",
    "        neg_items = np.where(items_by_user < 0.5)[0] # 얘는 미만 --> 상당히 수가 많이 나옴\n",
    "        neg_item_ids = np.random.choice(neg_items, min(num_neg_samples, len(neg_items)), replace=False) # net_ratio로 일부만 사용된 negative sample에서 랜덤하게 추출해서 사용\n",
    "        UIdataset[user_id].append(np.concatenate([pos_item_ids, neg_item_ids])) # positive sample과 negative sample을 concat해서 UIdataset[user_id]에 넣어줌\n",
    "        \n",
    "        # feature 추출 \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]): # positive, negative 두개를 concat한걸 item_id에 넣어서 append하는거 같은데...\n",
    "            features.append(user_features['age'][user_id]) # 여기에는 이상한게 item_id가 없음, 오타거나 안쓰거나 --> 아마 안쓰는거같음, 전체 for문에서 user_id (0, 1, 2, 3...)을 받아옴\n",
    "            # user_features = profile_df[['age']].to_dict()\n",
    "        UIdataset[user_id].append(np.array(features)) # 값들을 넣은 features를 array로 변경 후 user_id에 append\n",
    "        \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]): # 위와 마찬가지\n",
    "            features.append(item_features['genre_mid'][item_id]) # 여기서는 item_id를 씀, 위에서 추적한 (line 32, 37) profile_id들의 위치를 features로 사용함\n",
    "        UIdataset[user_id].append(np.array(features)) # 값들을 넣은 features를 array로 변경 후 user_id에 append\n",
    "        \n",
    "        # label 저장  \n",
    "        pos_labels = np.ones(len(pos_item_ids)) # positive 길이 만큼 1로 가득찬 array 생성\n",
    "        neg_labels = np.zeros(len(neg_item_ids)) # negative 길이 만큼 0으로 가득찬 array 생성\n",
    "        UIdataset[user_id].append(np.concatenate([pos_labels, neg_labels])) # 라벨들을 concat해서 user_id에 append\n",
    "\n",
    "        # 그럼 UIdataset[user_id]에는 \n",
    "        # positive, negative를 concat한 값\n",
    "        # 값들을 넣은 features들을 array로 변경한 값\n",
    "        # positive, negative길이 만큼의 0, 1이 들어간 값\n",
    "        # 이 4개가 들어감 (feature가 2개기 때문)\n",
    "    return UIdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "# realmakeUIdataset\n",
    "UIdataset = make_UIdataset(train, neg_ratio=cfg.neg_ratio) # neg_ratio = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15\n",
    "# makebatchdata\n",
    "def make_batchdata(user_indices, batch_idx, batch_size):\n",
    "    \"\"\" 배치 데이터로 변환 \n",
    "    Args:\n",
    "        user_indices : 전체 유저의 인덱스 정보 \n",
    "            ex) array([ 3100,  1800, 30098, ...,  2177, 11749, 20962])\n",
    "        batch_idx : 배치 인덱스 (몇번째 배치인지)\n",
    "            ex) 0 \n",
    "        batch_size : 배치 크기 \n",
    "            ex) 256 \n",
    "    Returns \n",
    "        batch_user_ids : 배치내의 유저 인덱스 정보 \n",
    "            ex) [22194, 22194, 22194, 22194, 22194, ...]\n",
    "        batch_item_ids : 배치내의 아이템 인덱스 정보 \n",
    "            ex) [36, 407, 612, 801, 1404, ...]\n",
    "        batch_feat0 : 배치내의 유저-아이템 인덱스 정보에 해당하는 feature0 정보 \n",
    "            ex) [6, 6, 6, 6, 6, ...]\n",
    "        batch_feat1 : 배치내의 유저-아이템 인덱스 정보에 해당하는 feature1 정보 \n",
    "            ex) [4,  4,  4, 23,  4, ...]\n",
    "        batch_labels : 배치내의 유저-아이템 인덱스 정보에 해당하는 label 정보 \n",
    "            ex) [1.0, 1.0, 1.0, 1.0, 1.0, ...]\n",
    "    \"\"\"\n",
    "    batch_user_indices = user_indices[batch_idx*batch_size : (batch_idx+1)*batch_size] # 안 건드려도 될거 같음\n",
    "    batch_user_ids = [] # 위 return 참고\n",
    "    batch_item_ids = []\n",
    "    batch_feat0 = []\n",
    "    batch_feat1 = []\n",
    "    batch_labels = []\n",
    "    for user_id in batch_user_indices: # batch_user_indices에 넣은 값들을 user_id로 하나씩 뽑음\n",
    "        item_ids = UIdataset[user_id][0] # positive, negative를 concat한 값, 13번 셀 (makeUIdateset) line 58 ~62 참고\n",
    "        feat0 = UIdataset[user_id][1] # 값들을 넣은 features1을 array로 변경한 값\n",
    "        feat1 = UIdataset[user_id][2] # 값들을 넣은 features2을 array로 변경한 값\n",
    "        labels = UIdataset[user_id][3] # positive, negative길이 만큼의 0, 1이 들어간 값\n",
    "        user_ids = np.full(len(item_ids), user_id) # full은 item_ids의 길이만한 array의 모든 값을 user_id로 채워줌\n",
    "        batch_user_ids.extend(user_ids.tolist()) # array를 list로 바꿔줌\n",
    "        batch_item_ids.extend(item_ids.tolist()) # array를 list로 바꿔줌\n",
    "        batch_feat0.extend(feat0.tolist()) # array를 list로 바꿔줌\n",
    "        batch_feat1.extend(feat1.tolist()) # array를 list로 바꿔줌\n",
    "        batch_labels.extend(labels.tolist()) # array를 list로 바꿔줌\n",
    "    return batch_user_ids, batch_item_ids, batch_feat0, batch_feat1, batch_labels\n",
    "\n",
    "def update_avg(curr_avg, val, idx): # 안 건드려도 될거 같음\n",
    "    \"\"\" 현재 epoch 까지의 평균 값을 계산 \n",
    "    \"\"\"\n",
    "    return (curr_avg * idx + val) / (idx + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 및 검증 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dS1EPRpeYU1W"
   },
   "outputs": [],
   "source": [
    "# 16\n",
    "# Learningandverification\n",
    "def train_epoch(cfg, model, optimizer, criterion): \n",
    "    model.train() # train mode로 변경해 줌\n",
    "    curr_loss_avg = 0.0 # 안 건드려도 될거 같음\n",
    "\n",
    "    user_indices = np.arange(cfg.n_users) # n_users(data.profile_id.max()+1)만큼의 요소를 가진 array 생성 (ex) np.arange(10) --> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "    np.random.RandomState(cfg.epoch).shuffle(user_indices) # cfg.epoch = 25, 지정해준 epoch값을 seed로 두고 user_indices를 섞어줌\n",
    "    \n",
    "    batch_num = int(len(user_indices) / cfg.batch_size) + 1 # batch_size = 256\n",
    "    # user_indices의 길이 --> 대략 33000, 이걸 256으로 나눈 것을 정수화 + 1\n",
    "    bar = tqdm(range(batch_num), leave=False) # leave는 한줄로 출력시킬지 말지 정하는 것 (True = 한줄), tqdm을 왜 쓴걸까 <-- enumerate(tqdm)를 그냥 같이 씀\n",
    "    for step, batch_idx in enumerate(bar): # 프로그레스 바를 표현하는 방식이 enumerate(tqdm)\n",
    "        user_ids, item_ids, feat0, feat1, labels = make_batchdata(user_indices, batch_idx, cfg.batch_size) # 15번 셀 (makebatchdata) line 3\n",
    "        # 배치 사용자 단위로 학습\n",
    "        user_ids = torch.LongTensor(user_ids).to(cfg.device) # 64비트의 부호 있는 정수는 torch.LongTensor를 사용, torch.to는 특정 device에 적당한 Tensor를 return해줌\n",
    "        item_ids = torch.LongTensor(item_ids).to(cfg.device)\n",
    "        feat0 = torch.FloatTensor(feat0).to(cfg.device)\n",
    "        feat1 = torch.LongTensor(feat1).to(cfg.device)\n",
    "        labels = torch.FloatTensor(labels).to(cfg.device)\n",
    "        labels = labels.view(-1, 1) # labels이라는 tensor를 2차원 (? X 1) 크기로 변경\n",
    "\n",
    "        # grad 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 forward\n",
    "        output = model.forward(user_ids, item_ids, [feat0, feat1]) # 12번 셀 (modeling) line 104\n",
    "        output = output.view(-1, 1) # output tensor를 2차원 (? X 1) 크기로 변경\n",
    "\n",
    "        loss = criterion(output, labels) # output과 labels의 loss 측정?\n",
    "        # criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        # 역전파\n",
    "        loss.backward() # '순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해갑니다'\n",
    "\n",
    "        # 최적화\n",
    "        optimizer.step()\n",
    "        if torch.isnan(loss):\n",
    "            print('Loss NAN. Train finish.')\n",
    "            break\n",
    "        curr_loss_avg = update_avg(curr_loss_avg, loss, step) # 15번 셀 (makebatchdata) line 43\n",
    "        \n",
    "        msg = f\"epoch: {cfg.epoch}, \"\n",
    "        msg += f\"loss: {curr_loss_avg.item():.5f}, \"\n",
    "        msg += f\"lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        bar.set_description(msg) # 그냥 message 보여주는 것 같음\n",
    "    rets = {'losses': np.around(curr_loss_avg.item(), 5)} # around = 주어진 소수 자리수로 고르게 반올림 함\n",
    "''' item함수 참고 --> dictionary에서 키값을 뽑아서 묶어서 반환해줌\n",
    "car = {\"name\" : \"BMW\", \"price\" : \"7000\"} \n",
    "car.items() \n",
    "dict_items([('name', 'BMW'), ('price', '7000')])\n",
    "'''\n",
    "    return rets # losses계산식 (curr_loss_avg)에 넣어서 계산 후 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17\n",
    "# Learningandverification2\n",
    "def recallk(actual, predicted, k = 25):\n",
    "    \"\"\" label과 prediction 사이의 recall 평가 함수 \n",
    "    Args:\n",
    "        actual : 실제로 본 상품 리스트\n",
    "        pred : 예측한 상품 리스트\n",
    "        k : 상위 몇개의 데이터를 볼지 (ex : k=25 상위 25개의 상품만 봄)\n",
    "    Returns: \n",
    "        recall_k : recall@k \n",
    "    \"\"\" \n",
    "    set_actual = set(actual) # set이라는 타입의 집합으로 변함, 중복이 제거됨\n",
    "    recall_k = len(set_actual & set(predicted[:k])) / min(k, len(set_actual)) # set 집합을 이용해 recall값을 구함, 안 건드려도 될거 같음\n",
    "    return recall_k\n",
    "\n",
    "def unique(sequence):\n",
    "    # preserves order\n",
    "    seen = set() # set하나 선언\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "    # for x in sequence <-- sequence에서 값을 뽑아서 x에 넣어줌\n",
    "        # if not (x in seen or seen.add(x)) <-- 중복을 제거해서 넣음? 이해 필요\n",
    "            # x # sequence에서 값을 뽑아서 seen에 넣고 x를 리스트에 넣어서 리턴해주는거 같은데, 해석 필요\n",
    "\n",
    "def ndcgk(actual, predicted, k = 25):\n",
    "    set_actual = set(actual)\n",
    "    idcg = sum([1.0 / np.log(i + 2) for i in range(min(k, len(set_actual)))])\n",
    "    # for i in range(min(k, len(set_actual)))\n",
    "        # sum([1.0 / np.log(i + 2)]) <-- np.log는 자연로그를 취해줌\n",
    "    dcg = 0.0\n",
    "    unique_predicted = unique(predicted[:k]) # k번째까지의 특징 추출\n",
    "    for i, r in enumerate(unique_predicted): # \n",
    "        if r in set_actual:\n",
    "            dcg += 1.0 / np.log(i + 2) # 진짜로 본거면 이걸 취해줌\n",
    "    ndcg_k = dcg / idcg # enumerate해서 로그 취해준걸, 아까 sum에 로그 취했던걸로 나눠줌\n",
    "    return ndcg_k\n",
    "\n",
    "def evaluation(gt, pred):\n",
    "    \"\"\" label과 prediction 사이의 recall, coverage, competition metric 평가 함수 \n",
    "    Args:\n",
    "        gt : 데이터 프레임 형태의 정답 데이터 \n",
    "        pred : 데이터 프레임 형태의 예측 데이터 \n",
    "    Returns: \n",
    "        rets : recall, ndcg, coverage, competition metric 결과 \n",
    "            ex) {'recall': 0.123024, 'ndcg': 056809, 'coverage': 0.017455, 'score': 0.106470}\n",
    "    \"\"\"    \n",
    "    gt = gt.groupby('profile_id')['album_id'].unique().to_frame().reset_index() # gt의 profile_id별 album_id의 특징(unique)을 뽑고 그 시리즈를 데이터프레임으로 변환(to_frame)\n",
    "                                                                                # 마지막으로 reset_index\n",
    "    gt.columns = ['profile_id', 'actual_list'] # gt의 column명 변경, id와 실제로 본 상품의 리스트가 들어있는 것 같음\n",
    "\n",
    "    evaluated_data = pd.merge(pred, gt, how = 'left', on = 'profile_id') # 예측값 기준 (profile_id 기준)으로 예측값과 gt를 merge\n",
    "# 이해 필요\n",
    "    evaluated_data['Recall@25'] = evaluated_data.apply(lambda x: recallk(x.actual_list, x.predicted_list), axis=1) # recallk는 line 3에 있음\n",
    "    # ['Recall@25'] 여따 오른쪽껄 삽입\n",
    "    # evaluated_data 여기 전체에 apply로 lambda 함수를 적용 --> lambda 함수는 recallk 함수를 갖고와서 사용\n",
    "    # lambda 함수의 input으로 들어간 x에서 실제 시청 값과 예측 값을 뽑아서 --> recallk에 넣고 (axis=1은 아마 열을 나타내는거 같음 (actual, predicted)) 밖으로 보내줌\n",
    "    evaluated_data['NDCG@25'] = evaluated_data.apply(lambda x: ndcgk(x.actual_list, x.predicted_list), axis=1)\n",
    "\n",
    "    recall = evaluated_data['Recall@25'].mean()\n",
    "    ndcg = evaluated_data['NDCG@25'] .mean() # ndcg참고\n",
    "    coverage = (evaluated_data['predicted_list'].apply(lambda x: x[:cfg.top_k]).explode().nunique())/meta_df.index.nunique()\n",
    "    # explode 사용\n",
    "    # top_k = 25, line 50의 데이터에 25번째까지 explode, nunique 적용 후 반환 후 meta_df의 인덱스 nunique로 나눠줌 <-- 이게 coverage\n",
    "    score = 0.75*recall + 0.25*ndcg # 여기부턴 그냥 보여주는거 같음\n",
    "    rets = {\"recall\" :recall, \n",
    "            \"ndcg\" :ndcg, \n",
    "            \"coverage\" :coverage, \n",
    "            \"score\" :score}\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQgUJbTOYU1X"
   },
   "outputs": [],
   "source": [
    "# 18\n",
    "# Learningandverification3\n",
    "def valid_epoch(cfg, model, data, mode='valid'):\n",
    "    pred_list = []\n",
    "    model.eval() # eval <-- 매개변수로 받은 값을 실행하는 함수\n",
    "    \n",
    "    query_user_ids = data['profile_id'].unique() # 추론할 모든 user array 집합\n",
    "    full_item_ids = np.array([c for c in range(cfg.n_items)]) # 추론할 모든 item array 집합 \n",
    "    full_item_ids_feat1 = [item_features['genre_mid'][c] for c in full_item_ids] # full_item_ids_feat1에 추론할 item array에서 추출한 값을 item_features['genre_mid'][c]\n",
    "                                                                                 # c번째에 넣어서 반환\n",
    "    for user_id in query_user_ids:\n",
    "        with torch.no_grad():\n",
    "            user_ids = np.full(cfg.n_items, user_id) # full은 n_itmes의 길이만한 array의 모든 값을 user_id로 채워줌\n",
    "            \n",
    "            user_ids = torch.LongTensor(user_ids).to(cfg.device) # 64비트의 부호 있는 정수는 torch.LongTensor를 사용, torch.to는 특정 device에 적당한 Tensor를 return해줌\n",
    "            item_ids = torch.LongTensor(full_item_ids).to(cfg.device)\n",
    "            \n",
    "            feat0 = np.full(cfg.n_items, user_features['age'][user_id])\n",
    "            feat0 = torch.FloatTensor(feat0).to(cfg.device) # 32-bit floating point는 torch.FloatTensor를 사용, torch.to는 특정 device에 적당한 Tensor를 return해줌\n",
    "                                                            # 아마 age는 숫자에 데이터가 들어있는 수치형 변수라서 이걸 쓴 것 같음, 정확하진 않음\n",
    "            feat1 = torch.LongTensor(full_item_ids_feat1).to(cfg.device)\n",
    "            \n",
    "            eval_output = model.forward(user_ids, item_ids, [feat0, feat1]).detach().cpu().numpy()\n",
    "            # detach는 tensor를 복사 (원본이 변하면 같이 변함)\n",
    "            \n",
    "            pred_u_score = eval_output.reshape(-1) #  = x.reshape(1, -1), 1차원 배열을 반환\n",
    "        \n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1] # argsort : 어레이 정렬, [::-1] <-- 거꾸로 출력\n",
    "        pred_u = full_item_ids[pred_u_idx] # line 8 과 line 28\n",
    "        pred_list.append(list(pred_u[:cfg.top_k])) # top_k = 25, \n",
    "        \n",
    "    pred = pd.DataFrame() # pred를 만들어 줌\n",
    "    pred['profile_id'] = query_user_ids\n",
    "    pred['predicted_list'] = pred_list\n",
    "    \n",
    "    # 모델 성능 확인 \n",
    "    if mode == 'valid':\n",
    "        rets = evaluation(data, pred) # 17번 셀 (Learningandverification2) line 37\n",
    "        return rets, pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-D5bJx7YU1X"
   },
   "source": [
    "### 하이퍼 파라미터 설정 & 최적화 기법 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19\n",
    "# 하이퍼파라미터\n",
    "# 하이퍼 파라미터 설정 \n",
    "cfg.batch_size = 256\n",
    "cfg.emb_dim = 256\n",
    "cfg.layer_dim = 256\n",
    "cfg.dropout = 0.05\n",
    "cfg.epochs = 25\n",
    "cfg.learning_rate = 0.0025\n",
    "cfg.reg_lambda = 0\n",
    "cfg.check_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "# settingmodeloptimizerloss\n",
    "# model 생성 및 optimizer, loss 함수 설정 \n",
    "model = NeuMF(cfg).to(cfg.device) # NeuMF에 적용해서 특정 device에 적당한 Tensor를 return해줌\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.reg_lambda)\n",
    "# 19번 셀 (하이퍼파라미터)에 있음\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eze0e7vtYU1Y"
   },
   "source": [
    "### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvhj3hD7YU1Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_logs = defaultdict(list) # key 1을 list로 준 딕셔너리를 생성\n",
    "best_scores  = 0 \n",
    "for epoch in range(cfg.epochs+1): # epoch = 25\n",
    "    cfg.epoch = epoch # cfg.epoch에 for로 돌아가는 값을 넣음\n",
    "    train_results = train_epoch(cfg, model, optimizer, criterion) # 16번 셀 (Learningandverification) line 3\n",
    "    \n",
    "    # cfg.check_epoch 번의 epoch 마다 성능 확인 \n",
    "    if epoch % cfg.check_epoch == 0: # check_epoch = 1, 1번마다 성능 확인\n",
    "        valid_results, _ = valid_epoch(cfg, model, valid) # 18번 셀 (Learningandverification3) line 3\n",
    "\n",
    "        logs = {\n",
    "            'Train Loss': train_results['losses'],\n",
    "            f'Valid Recall@{cfg.top_k}': valid_results['recall'],\n",
    "            f'Valid NDCG@{cfg.top_k}': valid_results['ndcg'],\n",
    "            'Valid Coverage': valid_results['coverage'],\n",
    "            'Valid Score': valid_results['score'],\n",
    "            } # 로그 출력\n",
    "\n",
    "        # 검증 성능 확인 \n",
    "        for key, value in logs.items(): # items --> dictionary 키값 쌍을 얻음 (item함수 참고 --> dictionary에서 키값을 뽑아서 묶어서 반환해줌)\n",
    "            # 16번 셀 (Learningandverification) line 47\n",
    "            total_logs[key].append(value) # total_logs <-- line 1, 여기 key번째에 value를 넣어줌\n",
    "\n",
    "        if epoch == 0: # ???, epoch마다 보여주는거 같음 아마\n",
    "            print(\"Epoch\", end=\",\")\n",
    "            print(\",\".join(logs.keys()))\n",
    "\n",
    "        print(f\"{epoch:02d}  \", end=\"\")\n",
    "        print(\"  \".join([f\"{v:0.6f}\" for v in logs.values()]))\n",
    "        \n",
    "        # 가장 성능이 좋은 가중치 파일을 저장 \n",
    "        if best_scores <= valid_results['score']: # pth파일 저장, # 안 건드려도 될거 같음\n",
    "            best_scores = valid_results['score']\n",
    "            torch.save(model.state_dict(), os.path.join(saved_path, 'model(best_scores).pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNVpn37RYU1Z"
   },
   "source": [
    "### 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = pd.DataFrame({'Train loss': total_logs['Train Loss']}) # 안 건드려도 될거 같음\n",
    "train_scores['Epoch'] = range(0, cfg.epochs+1, cfg.check_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( # 안 건드려도 될거 같음\n",
    "    ggplot(train_scores, aes(x='Epoch', y='Train loss'))\n",
    "        + geom_line(color='black') # line plot\n",
    "        + labs(x='Epoch', y='Train Loss')\n",
    "        + theme_light()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 안 건드려도 될거 같음\n",
    "valid_scores = pd.DataFrame(np.hstack([(range(0, cfg.epochs+1, cfg.check_epoch), total_logs[score], [score for i in range(0, cfg.epochs+1, cfg.check_epoch)]) for score in ['Valid Recall@25', 'Valid NDCG@25', 'Valid Coverage', 'Valid Score']])).T\n",
    "valid_scores.columns = ['Epoch', 'Score', 'Metric']\n",
    "valid_scores['Epoch'] = valid_scores['Epoch'].astype(int)\n",
    "valid_scores['Score'] = valid_scores['Score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( # 안 건드려도 될거 같음\n",
    "ggplot(valid_scores)  # here\n",
    "    + aes(\"Epoch\", \"Score\", color='Metric', group='Metric')\n",
    "    + geom_line()\n",
    "    + scale_y_continuous(breaks=[0.1*c for c in range(1, 10, 1)])\n",
    "    + theme_light()\n",
    "    + labs(x='Epoch', y='Valid Metric')\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07lHW5CAYU1F"
   },
   "source": [
    "## 제출 \n",
    "### 모든 유저에 대해 추천 결과 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(saved_path, 'model(best_scores).pth'))) # 안 건드려도 될거 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = os.path.join(data_path, 'sample_submission.csv') # 안 건드려도 될거 같음\n",
    "submission = pd.read_csv(submission_path)\n",
    "submission = valid_epoch(cfg, model, submission, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(output_path, 'submission.csv'), index = False) # 안 건드려도 될거 같음"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "기본과제-2_NCF+AutoRec (answer).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "964f46f3973c3d99dc6929d5bd1748275317def86f082ea2874e0d5a9cf6d261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
